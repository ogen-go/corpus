{
	"File": {
		"__typename": "FileMatch",
		"repository": {
			"name": "github.com/gbrueckl/Databricks.API.PowerShell"
		},
		"file": {
			"name": "account-2.0-aws.yaml",
			"size": 0,
			"path": "Docs/OpenAPISpecs/account-2.0-aws.yaml",
			"byteSize": 135795,
			"content": "openapi: 3.0.0\ninfo:\n  version: 2.0.0\n  title: Account API\n  description:\n    \"The Account API offers centralized management of multiple Databricks workspaces and related resources.\\n\n\n    Use this API for the following tasks:\\n\n\n    * **Create new workspaces** — This feature is available if your account is on the [E2 version of the platform](https://docs.databricks.com/getting-started/overview.html#e2-architecture) or on a select custom plan that allows multiple workspaces per account.\\n\n\n    * **Configure log delivery (billable usage or audit logs)** — This feature is Public Preview and works with all account IDs.\\n\n\n    * **Download billable usage logs** — This feature works with all account IDs.\\n\n\n    ### New workspaces\\n\n\n    Use this API to programmatically deploy, update, and delete workspaces. All workspaces have associated cloud credential configurations and storage configurations. This feature is available if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account. For additional details and steps, see\n    [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\\n\n\n    There are additional optional features that you can associate with your workspace for\n    deployment types and subscription types that support them. If you use these features\n    you will create additional configurations with this API. If you have questions about\n    availability, contact your Databricks representative:\\n\n\n    *  **[Customer-managed VPC](http://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html)** — Provide your own VPC. See the note below about regions for VPCs. To configure your workspace to use [AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) ([Public Preview](http://docs.databricks.com/release-notes/release-types.html)) for any type of connection, it is required that your workspace use a customer-managed VPC.\\n\n\n    *  **[Secure cluster connectivity](http://docs.databricks.com/security/secure-cluster-connectivity.html)** — Network architecture with no VPC open ports and no Databricks runtime worker public IP addresses. This is the default in workspaces for accounts on the E2 version of the platform as of September 1, 2020.\\n\n\n    * **[Customer-managed keys for managed services](http://docs.databricks.com/security/keys/customer-managed-keys-managed-services-aws.html)** — (Public Preview) Provide KMS keys to encrypt notebook and secret data in the control plane, as well as Databricks SQL queries and query history. This feature requires the Enterprise pricing tier and is available only for [some AWS regions](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html).\\n\n\n    * **[Customer-managed keys for storage](https://docs.databricks.com/security/keys/customer-managed-keys-storage-aws.html)** — (Public Preview) Provide KMS keys to encrypt the workspace's S3 bucket in the customer's AWS account, which contains the DBFS root and system data, as well as optionally encrypting cluster EBS instances. This feature requires the Enterprise pricing tier and is available only for [some AWS regions](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html). \\n\n\n    * **[AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)** - ([Public Preview](http://docs.databricks.com/release-notes/release-types.html)) AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises networks, without exposing the traffic to the public Internet. Workspaces on the E2 version of the platform support adding PrivateLink connections for two connection types. A front-end PrivateLink connection configure users to connect to the \u003cDatabricks\u003e web application or REST API over a PrivateLink endpoint. A back-end PrivateLink connection configures Databricks Runtime clusters connect to the control plane using two VPC endpoints (one for REST APIs, one for the secure cluster connectivity relay). PrivateLink is available only for some AWS regions. Your account must be enabled for PrivateLink to use these APIs. Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html). You can use the workspace creation API to add PrivateLink to a new workspace. If you want to add PrivateLink to an existing workspace, for the final workspace configuration update, you cannot use this API. Instead, contact your Databricks representative to make the change.\\n\n\n    To create a new workspace, use the [Create a workspace](#operation/create-workspace) operation. For detailed instructions of creating a new workspace with this API, see\n    [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\\n\n\n    To update an existing workspace, use the [Update a workspace](#operation/patch-workspace) operation. You can update only specific fields, and the set varies depending on whether it is a failed workspace or a running workspace.\n\n    ### Log delivery\\n\n\n    Use this API to manage log delivery. It supports two log types:\n    \n    * `BILLABLE_USAGE` — Learn more at [Configure billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html). For the CSV schema, see the [Usage page](https://docs.databricks.com/administration-guide/account-settings/usage.html).\\n\n\n    * `AUDIT_LOGS` — Learn more at [Configure audit logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html), including the JSON schema.\\n\n    \n    For the API reference, see [log delivery configurations](#tag/Log-delivery-configurations).\n\n    ### Download Billable Usage Logs\\n\n\n    Use this API to download a CSV file that contains billable usage logs for the specified account and date range.\\n\n\n    To programmatically process and analyze billable usage, Databricks recommends that you configure continuous billable usage delivery to an S3 bucket as documented on the [Deliver and access billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) page.\n    You can also use the Account API to directly download billable usage.\\n\n\n    See [Billable Usage Download](#tag/Billable-usage-download) for the API reference.\n\n    ### Configure Budgets\\n\n\n    Use this API to configure usage budgets for workspaces in your account, set up notifications for exceeding budget and query budget status.\n    \n    # Authentication\n\n    The Account API is different from most Databricks REST APIs:\n    \n    * It is published on `accounts.cloud.databricks.com`, not on the domain that matches your workspace URL. \n    \n    * Authenticate with your account name and password on the API, rather than a personal access token. For details, see [Create a new workspace using the Account API](https://docs.databricks.com/administration-guide/account-api/new-workspace.html).\n    \"\ntags:\n  - name: Credential configurations\n    description:\n      \"These APIs manage credential configurations for this workspace. Databricks needs\n      access to a cross-account service IAM role in your AWS account so that Databricks\n      can deploy clusters in the appropriate VPC for the new workspace. A credential\n      configuration encapsulates this role information, and its ID is used when creating a\n      new workspace.\"\n  - name: Storage configurations\n    description:\n      \"These APIs manage storage configurations for this workspace. A root storage S3\n      bucket in your account is required to store objects like cluster logs, notebook\n      revisions, and job results. You can also use the root storage S3 bucket for storage\n      of non-production DBFS data. A storage configuration encapsulates this bucket\n      information, and its ID is used when creating a new workspace. \"\n  - name: Network configurations\n    description:\n      \"These APIs manage network configurations for customer-managed VPCs (optional). A\n      network configuration encapsulates the IDs for AWS VPCs, subnets, and security\n      groups. Its ID is used when creating a new workspace if you use customer-managed\n      VPCs.\"\n  - name: Key configurations\n    description:\n      \"These APIs manage encryption key configurations for this workspace (optional). A key\n      configuration encapsulates the AWS KMS key information and some information about how the key\n      configuration can be used. There are two possible uses for key configurations:\\n\n\n      * Managed services: A key configuration can be used to encrypt a workspace's notebook and secret data in the control plane, as well as Databricks SQL queries and query history.\\n\n\n      * Storage: A key configuration can be used to encrypt a workspace's DBFS and EBS data in the data plane.\\n\n\n      In both of these cases, the key configuration's ID is used when creating a new workspace.\\n\n\n      This Preview feature is available if your account is on the E2 version of the platform. Updating a running workspace with workspace storage encryption requires that the workspace is on the E2 version of the platform. If you have an older workspace, it might not be on the E2 version of the platform. If you are not sure, contact your Databricks reprsentative.\"\n  - name: Workspaces\n    description:\n      \"These APIs manage workspaces for this account. A Databricks workspace is an environment for\n      accessing all of your Databricks assets. The workspace organizes objects (notebooks,\n      libraries, and experiments) into folders, and provides access to data and\n      computational resources such as clusters and jobs.\\n\n      \n      These endpoints are available if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\"\n  - name: Log delivery configurations\n    description:\n      \"These APIs manage log delivery configurations for this account. The two supported log types for this API are _billable usage logs_ and _audit logs_. This feature is in Public Preview. This feature works with all account ID types.\\n\n      \n      Log delivery works with all account types. However, if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you can optionally configure different storage destinations for each workspace.\\n\n\n      Log delivery status is also provided to know the latest status of log delivery attempts.\\n\n\n      The high-level flow of billable usage delivery:\\n\n\n      1. **Create storage**: In AWS, [create a new AWS S3 bucket](https://docs.databricks.com/administration-guide/account-api/aws-storage.html) with a specific bucket policy. Using Databricks APIs, call the Account API to create a [storage configuration object](#operation/create-storage-config) that uses the bucket name.\\n\n\n      2. **Create credentials**: In AWS, create the appropriate AWS IAM role. For full details, including the required IAM role policies and trust relationship, see [Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html). Using Databricks APIs, call the Account API to create a [credential configuration object](#operation/create-credential-config) that uses the IAM role's ARN. \\n\n\n      3. **Create log delivery configuration**: Using Databricks APIs, call the Account API to [create a log delivery configuration](#operation/create-log-delivery-config) that uses the credential and storage configuration objects from previous steps. You can specify if the logs should include all events of that log type in your account (_Account level_ delivery) or only events for a specific set of workspaces (_workspace level_ delivery). Account level log delivery applies to all current and future workspaces plus account level logs, while workspace level log delivery solely delivers logs related to the specified workspaces. You can create multiple types of delivery configurations per account.\\n\n\n      For billable usage delivery:\\n\n      \n      * For more information about billable usage logs, see [Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html). For the CSV schema, see the [Usage page](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n      \n      * The delivery location is `\u003cbucket-name\u003e/\u003cprefix\u003e/billable-usage/csv/`, where `\u003cprefix\u003e` is the name of the optional delivery path prefix you set up during log delivery configuration. Files are named `workspaceId=\u003cworkspace-id\u003e-usageMonth=\u003cmonth\u003e.csv`.\n      \n      * All billable usage logs apply to specific workspaces (_workspace level_ logs). You can aggregate usage for your entire account by creating an _account level_ delivery configuration that delivers logs for all current and future workspaces in your account.\n\n      * The files are delivered daily by overwriting the month's CSV file for each workspace.\\n\n\n      For audit log delivery:\\n\n      \n      * For more information about about audit log delivery, see [Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html), which includes information about the used JSON schema. \n      \n      * The delivery location is `\u003cbucket-name\u003e/\u003cdelivery-path-prefix\u003e/workspaceId=\u003cworkspaceId\u003e/date=\u003cyyyy-mm-dd\u003e/auditlogs_\u003cinternal-id\u003e.json`. Files may get overwritten with the same content multiple times to achieve exactly-once delivery.\n\n      * If the audit log delivery configuration included specific workspace IDs, only _workspace-level_ audit logs for those workspaces are delivered. If the log delivery configuration applies to the entire account (_account level_ delivery configuration), the audit log delivery includes workspace-level audit logs for all workspaces in the account as well as account-level audit logs. See [Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html) for details.\n\n      * Auditable events are typically available in logs within 15 minutes.\"\n  - name: \"Billable usage download\"\n    description:\n      \"This API allows you to download billable usage logs for the specified account and date range.\\n\n\n      This feature works with all account types.\"\n  - name: \"Budgets\"\n    description:\n      \"**Budgets feature is in [Private Preview](http://docs.databricks.com/release-notes/release-types.html).**\\n\n\n      These APIs manage budget configuration including notifications for exceeding a budget for a period. They can also retrieve the status of each budget.\n      \"\n  - name: \"AWS PrivateLink: private access settings\"\n    description:\n      \"**PrivateLink is in [Public Preview](http://docs.databricks.com/release-notes/release-types.html).**\\n \n      \n      These APIs manage private access settings for this account. A private access settings object specifies how your workspace is accessed using AWS PrivateLink. Each workspace that has any PrivateLink connections must include the ID for a private access settings object is in its workspace configuration object. Your account must be enabled for PrivateLink to use these APIs. Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\"\n  - name: \"AWS PrivateLink: VPC endpoint configurations\"\n    description:\n      \"**PrivateLink is in [Public Preview](http://docs.databricks.com/release-notes/release-types.html).**\\n\n\n      These APIs manage VPC endpoint configurations for this account. This object registers an AWS VPC endpoint in your Databricks account so your workspace can use it with AWS PrivateLink. Your VPC endpoint connects to one of two VPC endpoint services -- one for workspace (both for front-end connection and for back-end connection to REST APIs) and one for the back-end secure cluster connectivity relay from the data plane. Your account must be enabled for PrivateLink to use these APIs. Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\"\nservers:\n  - url: 'https://accounts.cloud.databricks.com/api/2.0'\n    description:\n      \"The Account API is published on `accounts.cloud.databricks.com` for all AWS\n      regional deployments.\"\npaths:\n  '/accounts/{account_id}/credentials':\n    parameters:\n      - $ref: '#/components/parameters/account_id_dual_use'\n    get:\n      summary: Get all credential configurations\n      operationId: get-credential-configs\n      description:\n        \"Get all Databricks credential configurations associated with an account specified\n        by ID.\"\n      tags:\n        - Credential configurations\n      responses:\n        '200':\n          description: Credential configurations were returned successfully.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListCredentialsResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create credential configuration\n      operationId: create-credential-config\n      description:\n        \"Create a Databricks credential configuration that represents cloud cross-account\n        credentials for a specified account. Databricks uses this to set up network\n        infrastructure properly to host Databricks clusters. For your AWS IAM role, you\n        need to trust the External ID (the Databricks Account API account ID)  in the returned credential object, and configure the\n        required access policy.\\n\n\n        Save the response's `credentials_id` field, which is the ID for your new\n        credential configuration object.\\n\n\n        For detailed instructions of creating a new workspace with this API, see\n        [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html)\"\n      tags:\n        - Credential configurations\n      requestBody:\n        description: Properties of the new credential configuration.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateCredentialRequest'\n      responses:\n        '201':\n          description: The credential configuration creation request succeeded.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Credential'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/credentials/{credentials_id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_dual_use'\n      - name: credentials_id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Databricks Account API credential configuration ID\n    get:\n      summary: Get credential configuration\n      operationId: get-credential-config\n      description:\n        \"Get a Databricks credential configuration object for an\n        account, both specified by ID.\"\n      tags:\n        - Credential configurations\n      responses:\n        '200':\n          description: The credential configuration was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Credential'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    delete:\n      summary: Delete credential configuration\n      operationId: delete-credential-config\n      description:\n        \"Delete a Databricks credential configuration object for an account, both\n        specified by ID. You cannot delete a credential that is associated with any\n        workspace.\"\n      tags:\n        - Credential configurations\n      responses:\n        '200':\n          description: The credential configuration was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Credential'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/storage-configurations':\n    parameters:\n      - $ref: '#/components/parameters/account_id_dual_use'\n    get:\n      summary: Get all storage configurations\n      operationId: get-storage-configs\n      description:\n        \"Get a list of all Databricks storage configurations for your\n         account, specified by ID.\"\n      tags:\n        - Storage configurations\n      responses:\n        '200':\n          description: The storage configurations were successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListStorageConfigurationsResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create new storage configuration\n      operationId: create-storage-config\n      description:\n        \"Create new storage configuration for an account, specified by ID. Uploads a storage configuration object that represents the root AWS S3 bucket\n        in your account. Databricks stores related workspace assets including DBFS,\n        cluster logs, and job results. For AWS S3 bucket, you need to configure the required bucket\n        policy.\\n\n\n        For detailed instructions of creating a new workspace with this API, see\n        [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html)\"\n      tags:\n        - Storage configurations\n      requestBody:\n        description: Properties of the new storage configuration.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateStorageConfigurationRequest'\n      responses:\n        '201':\n          description: The storage configuration was successfully created.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/StorageConfiguration'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/storage-configurations/{storage_configuration_id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_dual_use'\n      - name: storage_configuration_id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Databricks Account API storage configuration ID.\n    get:\n      summary: Get storage configuration\n      operationId: get-storage-config\n      description:\n        \"Get a Databricks storage configuration for an account, both specified by ID.\"\n      tags:\n        - Storage configurations\n      responses:\n        '200':\n          description: The storage configuration was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/StorageConfiguration'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    delete:\n      summary: Delete storage configuration\n      operationId: delete-storage-config\n      description:\n        \"Delete a Databricks storage configuration. You cannot delete a storage\n        configuration that is currently being associated to any workspace.\"\n      tags:\n        - Storage configurations\n      responses:\n        '200':\n          description: The storage configuration was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/StorageConfiguration'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/networks':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n    get:\n      summary: Get all network configurations\n      operationId: get-network-configs\n      description:\n        \"Get a list of all Databricks network configurations for an\n        account, specified by ID.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Network configurations\n      responses:\n        '200':\n          description: The network configurations were successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListNetworksResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create network configuration\n      operationId: create-network-config\n      description:\n        \"Create a Databricks network configuration that represents an\n        AWS VPC and its resources. The VPC will be used for new Databricks\n        clusters. This requires a pre-created VPC and subnets.\n        For VPC requirements, see\n        [Customer-managed VPC](http://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html).\\n\n\n        **Important**: You can share one customer-managed VPC with multiple workspaces in\n        a single account. Therefore, you can share one VPC across multiple Account API\n        network configurations. However, you **cannot** reuse subnets or Security Groups\n        between workspaces.  Because a Databricks Account API network configuration\n        encapsulates this information, you cannot reuse a Databricks Account API\n        network configuration across workspaces. If you plan to share one VPC with\n        multiple workspaces, be sure to size your VPC and subnets accordingly.\n\n        For detailed instructions of creating a new workspace with this API, see\n        [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Network configurations\n      requestBody:\n        description: Properties of the new network configuration.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateNetworkRequest'\n      responses:\n        '201':\n          description: The network configuration was successfully created.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/StorageConfiguration'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/networks/{network_id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n      - name: network_id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Databricks Account API network configuration ID.\n    get:\n      summary: Get a network configuration\n      operationId: get-network-config\n      description:\n        \"Get a Databricks network configuration, which represents an AWS VPC and its\n        resources.  This requires a pre-created VPC and subnets. For VPC requirements, see\n        [Customer-managed\n        VPC](http://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html).\\n\n        \n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Network configurations\n      responses:\n        '200':\n          description: The network configuration was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Network'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    delete:\n      summary: Delete network configuration\n      operationId: delete-network-config\n      description:\n        \"Delete a Databricks network configuration, which represents an AWS VPC and its\n        resources. You cannot delete a network that is associated with a workspace.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Network configurations\n      responses:\n        '200':\n          description: The network configuration was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Network'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/customer-managed-keys':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n    get:\n      summary: Get all encryption key configurations\n      operationId: get-key-configs\n      description:\n        \"Get all customer-managed key configuration objects for an account. If the key is specified as a\n        workspace's managed services customer-managed key, Databricks will use the key to encrypt\n        the workspace's notebooks and secrets in the control plane, as well as Databricks SQL queries and query history. If the key is specified as a workspace's\n        storage customer-managed key, the key is used to encrypt the workspace's root S3 bucket and optionally can encrypt cluster EBS volumes\n        data in the data plane.\\n\n\n        **Important**: Customer-managed keys are supported only for some deployment types,\n        subscription types, and AWS regions.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Key configurations\n      responses:\n        '200':\n          description: The encryption key configurations were successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListCustomerManagedKeysResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create encryption key configuration\n      operationId: create-key-config\n      description:\n        \"Create a customer-managed key configuration object for an account, specified by\n        ID. This operation uploads a reference to a customer-managed key to Databricks. If the key is assigned\n        as a workspace's customer-managed key for managed services, Databricks uses the key to\n        encrypt the workspaces notebooks and secrets in the control plane, as well as Databricks SQL queries and query history. If it is specified as a\n        workspace's customer-managed key for workspace storage, the key encrypts the workspace's\n        root S3 bucket (which contains the workspace's root DBFS and system data) and optionally cluster EBS volume data.\\n\n\n        **Important**: Customer-managed keys are supported only for some deployment types,\n        subscription types, and AWS regions.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\"\n      tags:\n        - Key configurations\n      requestBody:\n        description: Properties of the encryption key configuration.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateCustomerManagedKeyRequest'\n            examples:\n              All:\n                value:\n                  aws_key_info:\n                    key_arn: \"arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\"\n                    key_alias: \"alias/projectKey\"\n                    reuse_key_for_cluster_volumes: true\n                  use_cases: [MANAGED_SERVICES, STORAGE]\n              Storage:\n                value:\n                  aws_key_info:\n                    key_arn: \"arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\"\n                    key_alias: \"alias/projectKey\"\n                    reuse_key_for_cluster_volumes: false\n                  use_cases: [STORAGE]\n              Managed Services:\n                value:\n                  aws_key_info:\n                    key_arn: \"arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\"\n                    key_alias: \"alias/projectKey\"\n                  use_cases: [MANAGED_SERVICES]\n      responses:\n        '201':\n          description: The encryption key configuration was successfully created.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/CustomerManagedKey'\n              examples:\n                All:\n                  value:\n                    customer_managed_key_id: \"680290f4-6931-497c-a6b4-514f6694e228\"\n                    account_id: \"449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65\"\n                    aws_key_info:\n                      key_arn: \"arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\"\n                      key_alias: \"alias/projectKey\"\n                      key_region: us-west-2\n                      reuse_key_for_cluster_volumes: true\n                    creation_time: 0\n                    use_cases: [MANAGED_SERVICES, STORAGE]\n                Storage:\n                  value:\n                    customer_managed_key_id: \"680290f4-6931-497c-a6b4-514f6694e228\"\n                    account_id: \"449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65\"\n                    aws_key_info:\n                      key_arn: \"arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\"\n                      key_alias: \"alias/projectKey\"\n                      key_region: us-west-2\n                      reuse_key_for_cluster_volumes: false\n                    creation_time: 0\n                    use_cases: [STORAGE]\n                Managed Services:\n                  value:\n                    customer_managed_key_id: \"680290f4-6931-497c-a6b4-514f6694e228\"\n                    account_id: \"449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65\"\n                    aws_key_info:\n                      key_arn: \"arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\"\n                      key_alias: \"alias/projectKey\"\n                      key_region: us-west-2\n                    creation_time: 0\n                    use_cases: [MANAGED_SERVICES]\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '403':\n          $ref: '#/components/responses/Forbidden'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/customer-managed-keys/{customer_managed_key_id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n      - name: customer_managed_key_id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Databricks encryption key configuration ID.\n    get:\n      summary: Get encryption key configuration\n      operationId: get-key-config\n      description:\n        \"Get a customer-managed key configuration object for an account, specified by\n        ID. This operation uploads a reference to a customer-managed key to Databricks. If assigned\n        as a workspace's customer-managed key for managed services, Databricks uses the key to\n        encrypt the workspaces notebooks and secrets in the control plane, as well as Databricks SQL queries and query history. If it is specified as a\n        workspace's customer-managed key for storage, the key encrypts the workspace's\n        root S3 bucket (which contains the workspace's root DBFS and system data) and optionally cluster EBS volume data.\\n\n\n        **Important**: Customer-managed keys are supported only for some deployment types,\n        subscription types, and AWS regions.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Key configurations\n      responses:\n        '200':\n          description: The encryption key configuration was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/CustomerManagedKey'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    delete:\n      summary: Delete encryption key configuration\n      operationId: delete-key-config\n      description:\n        \"Delete a customer-managed key configuration object for an account.\n        You cannot delete a configuration that is associated with a running\n        workspace.\"\n      tags:\n        - Key configurations\n      responses:\n        '200':\n          description: The encryption key configuration was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/CustomerManagedKey'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/customer-managed-key-history':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n    get:\n      summary: Get history of a key's associations with workspaces\n      operationId: get-key-workspace-history\n      description:\n        \"Get a list of records of how key configurations were associated with workspaces.\\n\n\n        **Important**: Customer-managed keys are supported only for some deployment types, subscription types, and AWS regions.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Key configurations\n      responses:\n        '200':\n          description: The key's workspace association history was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListWorkspaceEncryptionKeyRecordsResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/workspaces':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n    get:\n      summary: Get all workspaces\n      operationId: get-workspaces\n      description:\n        \"Get a list of all workspaces associated with an account, specified by ID.\\n\n\n        This operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\"\n      tags:\n        - Workspaces\n      responses:\n        '200':\n          description: The workspaces were returned successfully.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListWorkspacesResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create a new workspace\n      operationId: create-workspace\n      description:\n        \"Create a new workspace using a credential configuration and a storage\n        configuration, an optional network configuration (if using a customer-managed\n        VPC), an optional managed services key configuration (if using customer-managed keys for\n        managed services), and an optional storage key configuration (if using customer-managed keys for\n        storage). The key configurations used for managed services and storage encryption may be the\n        same or different.\\n\n\n        **Important**: This operation is asynchronous. A response with HTTP status code 200\n        means the request has been accepted and is in progress, but does not mean that the\n        workspace deployed successfully and is running. The initial workspace status is\n        typically  `PROVISIONING`. Use the workspace ID (`workspace_id`) field in the\n        response to identify the new workspace and make repeated `GET` requests with the\n        workspace ID and check its status. The workspace becomes available when the status\n        changes to `RUNNING`.\\n\n\n        You can share one customer-managed VPC with multiple workspaces in\n        a single account. It is not required to create a new VPC for each workspace.\n        However, you **cannot** reuse subnets or Security Groups between workspaces. If\n        you plan to share one VPC with multiple workspaces, be sure to size your VPC and\n        subnets accordingly. Because a Databricks Account API network configuration\n        encapsulates this information, you cannot reuse a Databricks Account API\n        network configuration across workspaces.\n\n        For detailed instructions of creating a new workspace with this API **including\n        error handling** see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\\n\n\n        **Important**: Customer-managed VPCs, PrivateLink, and customer-managed keys are supported on a limited set of deployment and subscription types. If you have questions about availability, contact your Databricks representative.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\"\n      tags:\n        - Workspaces\n      requestBody:\n        description: Properties of the new workspace.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateWorkspaceRequest'\n      responses:\n        '201':\n          description: Workspace creation request was received. Check workspace status.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Workspace'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '403':\n          $ref: '#/components/responses/Forbidden'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/workspaces/{workspace_id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n      - name: workspace_id\n        in: path\n        required: true\n        schema:\n          type: integer\n          format: int64\n        description: Workspace ID.\n    get:\n      summary: Get workspace\n      operationId: get-workspace\n      description:\n        \"Get information including status for a Databricks workspace, specified by ID. In\n        the response, the `workspace_status` field indicates the current status. After\n        initial workspace creation (which is asynchronous), make repeated `GET` requests\n        with the workspace ID and check its status. The workspace becomes available when\n        the status changes to `RUNNING`.\\n\n\n        For detailed instructions of creating a new workspace with this API **including\n        error handling** see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\\n\n        \n        This operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\"\n      tags:\n        - Workspaces\n      responses:\n        '200':\n          description: The workspace configuration was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Workspace'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    patch:\n      summary: Update workspace configuration\n      operationId: patch-workspace\n      description:\n        \"The `PATCH` operation on this endpoint can update a workspace configuration for either a running workspace or a failed workspace. The elements that can be updated varies between these two use cases.\\n\n\n        ## Failed Workspaces\n\n        You can update a Databricks workspace configuration for failed workspace deployment for some but not all fields. This request supports updating only the following fields of a failed workspace:\n\n        - AWS region\n\n        - Credential configuration ID\n\n        - Storage configuration ID\n\n        - Network configuration ID. Used only if you use customer-managed VPC.\n\n        - Key configuration ID for managed services. Used only if you use customer-managed keys\n        for managed services.\n\n        - Key configuration ID for workspace storage . Used only if you use customer-managed keys for workspace storage.\\n\n\n        After calling the `PATCH` operation to update the workspace configuration, make repeated `GET` requests with the workspace ID and check the workspace status. The workspace is successful if the status changes to `RUNNING`.\\n\n\n        For detailed instructions of creating a new workspace with this API **including error handling** see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\\n\n\n        ## Running Workspaces\n\n        You can update a Databricks workspace configuration for running workspaces for some but not all fields. This request supports updating only the following fields of a running workspace:\n\n        - Credential configuration ID\\n\n\n        - Network configuration ID. Used only if you already use use customer-managed VPC. This change is supported only if you specified a network configuration ID in your original workspace creation. In other words, you cannot switch from a Databricks-managed VPC to a customer-managed VPC. Note: You cannot use a network configuration update in this API to add support for PrivateLink (in Public Preview). To add PrivateLink to an existing workspace, contact your Databricks representative. \\n\n\n        - Key configuration ID for workspace storage. You can set this only if the workspace does not already have a customer-managed key configuration for workspace storage. \\n\n\n        **Important**: For updating running workspaces, this API is unavailable on Mondays, Tuesdays, and Thursdays from 4:30pm-7:30pm PST due to routine maintenance. Plan your workspace updates accordingly. For questions about this schedule, contact your Databricks representative.\\n\n\n        **Important**: To update a running workspace, your workspace must have no running cluster instances, which includes all-purpose clusters, job clusters, and pools that may have running clusters. Terminate all cluster instances in the workspace before calling this API. \\n\n\n        After calling the `PATCH` operation to update the workspace configuration, make repeated `GET` requests with the workspace ID and check the workspace status and the status of the fields.\n        \n        * For workspaces with a Databricks-managed VPC, the workspace status becomes `PROVISIONING` temporarily (typically under 20 minutes). If the workspace update is successful, the workspace status changes to `RUNNING`. Note that you can also check the workspace status in the [Account Console](https://docs.databricks.com/administration-guide/account-settings-e2/account-console-e2.html). However, you cannot use or create clusters for another 20 minutes after that status change. This results in a total of up to 40 minutes in which you cannot create clusters. If you create or use clusters before this time interval elapses, clusters do not launch successfully, fail, or could cause other unexpected behavior.  \\n\n        \n        * For workspaces with a customer-managed VPC, the workspace status stays at status `RUNNING` and the VPC change happens immediately. A change to the storage customer-managed key configuration ID may take a few minutes to update, so continue to check the workspace until you observe it has updated. If the update fails, the workspace may revert silently to its original configuration. Once the workspace has updated, you cannot use or create clusters for another 20 minutes. If you create or use clusters before this time interval elapses, clusters do not launch successfully, fail, or could cause other unexpected behavior.\\n\n\n        If you update the storage customer-managed key configuration ID, it takes 20 minutes for the changes to fully take effect. During the 20 minute wait, if you make regular calls to the DBFS API, it is important that you stop all REST API calls to the DBFS API during this time. \\n\n\n        **Important**: Customer-managed keys and customer-managed VPCs are supported by only some deployment types and subscription types. If you have questions about availability, contact your Databricks representative.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\"\n      tags:\n        - Workspaces\n      requestBody:\n        description: Changes of the workspace properties.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UpdateWorkspaceRequest'\n      responses:\n        '200':\n          description: The workspace update request is accepted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Workspace'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '403':\n          $ref: '#/components/responses/Forbidden'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n        '509':\n          $ref: '#/components/responses/ServiceUnavailable'\n    delete:\n      summary: Delete workspace\n      operationId: delete-workspace\n      description:\n        \"Terminate and delete a Databricks workspace. From an API perspective, deletion is\n        immediate. However, it may take a few minutes for all workspaces resources to be\n        deleted, depending on the size and number of workspace resources.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\"\n      tags:\n        - Workspaces\n      responses:\n        '200':\n          description: The workspace was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Workspace'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/workspaces/{workspace_id}/customer-managed-key-history':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n      - name: workspace_id\n        in: path\n        required: true\n        schema:\n          type: integer\n          format: int64\n        description: Workspace ID.\n    get:\n      summary: Get the history of a workspace's associations with keys\n      operationId: get-workspace-key-history\n      description:\n        \"Given a workspace specified by ID, this request gets a list of all associations\n        with key configuration objects that encapsulate customer-managed keys that encrypt\n        managed services, workspace storage, or in some cases both.\\n\n\n        **Important**: In the current implementation, keys cannot be rotated or removed from a\n        workspace. It is possible for a workspace to show a storage customer-managed key having been\n        attached and then detached if the workspace was updated to use the key and the update\n        operation failed.\\n\n\n        **Important**: Customer-managed keys are supported only for some deployment types and\n        subscription types.\\n\n        \n        This operation is available only if your account is on the E2 version of the platform.\"\n      tags:\n        - Workspaces\n      responses:\n        '200':\n          description: The workspace's key history was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListWorkspaceEncryptionKeyRecordsResponse'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/log-delivery':\n    parameters:\n      - $ref: '#/components/parameters/account_id'\n    get:\n      parameters:\n        - name: status\n          in: query\n          description: Filter by status `ENABLED` or `DISABLED`.\n          schema:\n            $ref: '#/components/schemas/LogDeliveryConfigStatus'\n        - name: credentials_id\n          in: query\n          description: Filter by credential configuration ID.\n          schema:\n            type: string\n        - name: storage_configuration_id\n          in: query\n          description: Filter by storage configuration ID.\n          schema:\n            type: string\n      summary: Get all log delivery configurations\n      operationId: get-log-delivery-configs\n      description:\n        \"Get all Databricks log delivery configurations associated with an\n        account specified by ID.\"\n      tags:\n        - Log delivery configurations\n      responses:\n        '200':\n          description: Log delivery configurations were returned successfully.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/WrappedLogDeliveryConfigurations'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create a new log delivery configuration\n      operationId: create-log-delivery-config\n      description:\n        \"Create a new Databricks log delivery configuration to enable delivery of the specified type of logs to your storage location. This requires that you already created a [credential object](#operation/create-credential-config) (which encapsulates a cross-account service IAM role) and a [storage configuration object](#operation/create-storage-config) (which encapsulates an S3 bucket).\\n\n\n        For full details, including the required IAM role policies and bucket policies, see [Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) or [Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\\n\n\n        Note: There is a limit on the number of log delivery configurations available per account (each limit applies separately to each log type including billable usage and audit logs). You can create a maximum of two enabled account-level delivery configurations (configurations without a workspace filter) per type. Additionally, you can create two enabled workspace level delivery configurations per workspace for each log type, meaning the same workspace ID can occur in the workspace filter for no more than two delivery configurations per log type.\\n\n\n        You cannot delete a log delivery configuration, but you can disable it (see [Enable or disable log delivery configuration](#operation/patch-log-delivery-config-status)).\"\n\n      tags:\n        - Log delivery configurations\n      requestBody:\n        description: Properties of the new log delivery configuration.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/WrappedCreateLogDeliveryConfiguration'\n      responses:\n        '200':\n          description: The log delivery configuration creation request succeeded.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/WrappedLogDeliveryConfiguration'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/log-delivery/{log_delivery_configuration_id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id'\n      - name: log_delivery_configuration_id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Databricks log delivery configuration ID\n    get:\n      summary: Get log delivery configuration\n      operationId: get-log-delivery-config\n      description: \u003e-\n        Get a Databricks log delivery configuration object for an account, both\n        specified by ID.\n      tags:\n        - Log delivery configurations\n      responses:\n        '200':\n          description: The log delivery configuration was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/WrappedLogDeliveryConfiguration'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    patch:\n      summary: Enable or disable log delivery configuration\n      operationId: patch-log-delivery-config-status\n      description: \u003e-\n        Enable or disable a log delivery configuration. Deletion of delivery configurations is not supported, so disable log delivery configurations that are no longer needed.\n        Note that you can't re-enable a delivery configuration if this would violate the delivery configuration limits described under [Create log delivery](#operation/create-log-delivery-config).\n\n      tags:\n        - Log delivery configurations\n      requestBody:\n        description: The new status for this log delivery configuration object.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UpdateLogDeliveryConfigurationStatusRequest'\n      responses:\n        '200':\n          description: The log delivery configuration was successfully updated.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/WrappedLogDeliveryConfiguration'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/usage/download':\n    parameters:\n      - $ref: '#/components/parameters/account_id'\n    get:\n      parameters:\n        - name: start_month\n          in: query\n          description: \"Format: `YYYY-MM`. First month to return billable usage logs for. This field is required.\"\n          schema:\n            $ref: '#/components/schemas/UsageDownloadMonth'\n          required: true\n        - name: end_month\n          in: query\n          description: \"Format: `YYYY-MM`. Last month to return billable usage logs for. This field is required.\"\n          schema:\n            $ref: '#/components/schemas/UsageDownloadMonth'\n          required: true\n        - name: personal_data\n          in: query\n          description: Specify whether to include personally identifiable information in the billable usage logs, for example the email addresses of cluster creators. Handle this information with care. Defaults to false.\n          schema:\n            type: boolean\n          default: false\n      summary: Return billable usage logs in CSV format for the specified account and date range.\n      operationId: download-billable-usage\n      description:\n        \"Return billable usage logs in CSV format for the specified account and date range. See [CSV file schema](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html#csv-file-schema) for the data schema. Note that this method may take multiple seconds to complete.\"\n      tags:\n        - Billable usage download\n      responses:\n        '200':\n          description: Billable usage data was returned successfully.\n          content:\n            application/csv:\n              schema:\n                type: string\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/budget':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n    post:\n      summary: Create a new budget\n      operationId: create-budget\n      tags:\n        - Budgets\n      description:\n        \"Create a new budget in this account.\"\n      requestBody:\n        description: Properties of the new budget\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              required:\n                - budget\n              properties:\n                budget:\n                  $ref: '#/components/schemas/BudgetCreateRequest'\n      responses:\n        '200':\n          description: The budget was successfully created.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/BudgetWithStatus'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    get:\n      summary: Get all budgets associated with this account\n      operationId: get-budgets\n      tags:\n        - Budgets\n      description:\n        \"Get all budgets associated with this account, including non-cumulative status for each day the budget is configured for.\"\n      responses:\n        '200':\n          description: The list of budgets was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/BudgetList'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/budget/{budget_id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n      - name: budget_id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Budget ID\n    get:\n      summary: Get budget and its status\n      operationId: get-budget\n      tags:\n        - Budgets\n      description:\n        \"Get budget specified by its UUID, including non-cumulative status for each day the budget is configured for.\"\n      responses:\n        '200':\n          description: The budget was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/BudgetWithStatus'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    delete:\n      summary: Delete budget\n      operationId: delete-budget\n      tags:\n        - Budgets\n      description:\n        \"Delete budget specified by its UUID.\"\n      responses:\n        '200':\n          description: The budget that was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/BudgetWithStatus'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    patch:\n      summary: Modify a budget\n      operationId: modify-budget\n      tags:\n        - Budgets\n      description:\n        \"Modify a budget in this account. Budget properties will be fully overwritten.\"\n      requestBody:\n        description: Properties to set the budget to\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              required:\n                - budget\n              properties:\n                budget:\n                  $ref: '#/components/schemas/BudgetCreateRequest'\n  '/accounts/{account_id}/private-access-settings':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n    get:\n      summary: Get all private access settings objects\n      operationId: get-private-access-settings-objects\n      description:\n        \"Get a list of all private access settings objects for an account, specified by ID.\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: private access settings\"\n      responses:\n        '200':\n          description: The private access settings object was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListPrivateAccessSettingsResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create private access settings\n      operationId: create-private-access-settings\n      description:\n        \"Create a private access settings object, which specifies how your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink). To use AWS PrivateLink, a workspace must have a private access settings object referenced by ID in the workspace's `private_access_settings_id` property.\\n\n\n        You can share one private access settings with multiple workspaces in a single account.However, private access settings are region specific, so only workspaces in the same region may use a given private access settings object.\\n\n        \n        Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: private access settings\"\n      requestBody:\n        description: Properties of the new private access settings object.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UpsertPrivateAccessSettingsRequest'\n      responses:\n        '200':\n          description: The private access settings object was successfully created.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/PrivateAccessSettings'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/private-access-settings/{private-access-settings-id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n      - name: private-access-settings-id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Databricks Account API private access settings ID.\n    get:\n      summary: Get a private access settings object\n      operationId: get-private-access-settings-object\n      description:\n        \"Get a private access settings object, which specifies how your workspace\n         is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).\\n\n         \n         Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: private access settings\"\n      responses:\n        '200':\n          description: The private access settings object was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/PrivateAccessSettings'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    delete:\n      summary: Delete a private access settings object\n      operationId: delete-private-access-settings-object\n      description:\n        \"Delete a private access settings object, which determines how your workspace\n         is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).\\n\n\n         Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: private access settings\"\n      responses:\n        '200':\n          description: The private access settings was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/PrivateAccessSettings'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    put:\n      summary: Replace private access settings\n      operationId: replace-private-access-settings\n      description:\n        \"Update an existing private access settings object, which specifies how your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink). To use AWS PrivateLink, a workspace must have a private access settings object referenced by ID in the workspace's `private_access_settings_id` property.\\n\n\n        This operation fully overwrites your existing private access settings object attached to your workspaces. All\n        workspaces attached to the private access settings will see the effects of any change. If updating `public_access_enabled`,\n        `private_access_level`, or `allowed_vpc_endpoint_ids`, effects of the change may take a couple minutes to propagate\n       to the workspace API.\n\n        You can share one private access settings with multiple workspaces in a single account. However, private access settings are region specific, so only workspaces in the same region may use a given private access settings object.\\n\n\n        Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: private access settings\"\n      requestBody:\n        description: Properties of the new private access settings object.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UpsertPrivateAccessSettingsRequest'\n      responses:\n        '200':\n          description: The private access settings object was successfully updated.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/PrivateAccessSettings'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/vpc-endpoints':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n    get:\n      summary: Get all VPC endpoint configurations\n      operationId: get-vpc-endpoints\n      description:\n        \"Get a list of all VPC endpoints for an account, specified by ID.\\n\n\n        Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: VPC endpoint configurations\"\n      responses:\n        '200':\n          description: The VPC endpoints were successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ListVPCEndpointsResponse'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    post:\n      summary: Create VPC endpoint configuration\n      operationId: create-vpc-endpoint\n      description:\n        \"Create a VPC endpoint configuration, which represents a [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html) object in AWS used to communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).\\n\n\n        **IMPORTANT**: When you register a VPC endpoint to the Databricks workspace VPC endpoint service for any workspace, **in this release \u003cDatabricks\u003e enables front-end (web application and REST API) access from the source network of the VPC endpoint to all workspaces in that AWS region in your \u003cDatabricks\u003e account if the workspaces have any PrivateLink connections in their workspace configuration**. If you have questions about this behavior, contact your Databricks representative.\\n\n\n        Within AWS, your VPC endpoint stays in `pendingAcceptance` state until you register it in a VPC endpoint configuration through the Account API. Upon doing so, the Databricks [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html) automatically accepts the VPC endpoint and it eventually transitions to the `available` state.\\n\n\n        Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: VPC endpoint configurations\"\n      requestBody:\n        description: Properties of the new VPC endpoint configuration.\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateVPCEndpointRequest'\n      responses:\n        '200':\n          description: The VPC endpoint configuration was successfully created.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/VPCEndpoint'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n  '/accounts/{account_id}/vpc-endpoints/{vpc-endpoint-id}':\n    parameters:\n      - $ref: '#/components/parameters/account_id_e2'\n      - name: vpc-endpoint-id\n        in: path\n        required: true\n        schema:\n          type: string\n          format: uuid\n        description: Databricks VPC endpoint ID.\n    get:\n      summary: Get a VPC endpoint configuration\n      operationId: get-vpc-endpoint-config\n      description:\n        \"Get a VPC endpoint configuration, which represents a [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html) object in AWS used to communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: VPC endpoint configurations\"\n      responses:\n        '200':\n          description: The VPC endpoint was successfully returned.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/VPCEndpoint'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '500':\n          $ref: '#/components/responses/InternalError'\n    delete:\n      summary: Delete VPC endpoint configuration\n      operationId: delete-vpc-endpoint\n      description:\n        \"Delete a VPC endpoint configuration, which represents an [AWS VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html) that can communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).\\n\n\n        Upon deleting a VPC endpoint configuration, the VPC endpoint in AWS changes its state from `accepted` to `rejected`, meaning it will no longer be usable from your VPC.\\n\n\n        Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\\n\n\n        This operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.\"\n      tags:\n        - \"AWS PrivateLink: VPC endpoint configurations\"\n      responses:\n        '200':\n          description: The VPC endpoint configuration was successfully deleted.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/PrivateAccessSettings'\n        '401':\n          $ref: '#/components/responses/Unauthenticated'\n        '404':\n          $ref: '#/components/responses/NotFound'\n        '409':\n          $ref: '#/components/responses/Conflict'\n        '500':\n          $ref: '#/components/responses/InternalError'\ncomponents:\n  parameters:\n    account_id_dual_use:\n      name: account_id\n      in: path\n      required: true\n      schema:\n        type: string\n        format: uuid\n      description: Databricks account ID. When you create or manage workspaces, your account must be on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account. If you are configuring log delivery, all account types are supported. For non-E2 account types, get your account ID from the [Accounts Console](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n    account_id_e2:\n      name: account_id\n      in: path\n      required: true\n      schema:\n        type: string\n        format: uuid\n      description: Databricks account ID. Your account must be on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.\n    account_id:\n      name: account_id\n      in: path\n      required: true\n      schema:\n        type: string\n        format: uuid\n      description: Databricks account ID of any type. For non-E2 account types, get your account ID from the [Accounts Console](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n  schemas:\n    Credential:\n      type: object\n      properties:\n        credentials_id:\n          type: string\n          format: uuid\n          description: Databricks credential configuration ID.\n        credentials_name:\n          type: string\n          description: The human-readable name of the credential configuration object.\n          minLength: 4\n          maxLength: 256\n        aws_credentials:\n          type: object\n          properties:\n            sts_role:\n              type: object\n              properties:\n                role_arn:\n                  type: string\n                  description: The Amazon Resource Name (ARN) of the cross-account role.\n                external_id:\n                  type: string\n                  description: \u003e-\n                    The external ID that needs to be trusted by the cross-account role. This\n                    is always your Databricks account ID.\n        account_id:\n          type: string\n          format: uuid\n          description:\n            \"The Databricks account ID that hosts the credential.\"\n        creation_time:\n          type: integer\n          format: int64\n          description: Time in epoch milliseconds when the credential was created.\n    CreateCredentialRequest:\n      type: object\n      required:\n        - credentials_name\n        - aws_credentials\n      properties:\n        credentials_name:\n          type: string\n          description: The human-readable name of the credential configuration object.\n          example: credential_1\n          minLength: 4\n          maxLength: 256\n        aws_credentials:\n          type: object\n          properties:\n            sts_role:\n              type: object\n              properties:\n                role_arn:\n                  type: string\n                  description: The Amazon Resource Name (ARN) of the cross account role.\n                  example: 'arn-aws-iam::111110000000:role/test_role'\n    ListCredentialsResponse:\n      description: List of credential configuration objects.\n      type: array\n      items:\n        $ref: '#/components/schemas/Credential'\n    StorageConfiguration:\n      type: object\n      properties:\n        storage_configuration_id:\n          type: string\n          format: uuid\n          description: Databricks storage configuration ID.\n        storage_configuration_name:\n          type: string\n          description: The human-readable name of the storage configuration.\n          minLength: 4\n          maxLength: 256\n        root_bucket_info:\n          description: \"Root S3 bucket information.\"\n          type: object\n          properties:\n            bucket_name:\n              type: string\n              description: The name of the S3 bucket.\n        account_id:\n          type: string\n          format: uuid\n          description:\n            \"The Databricks account ID that hosts the credential.\"\n        creation_time:\n          type: integer\n          format: int64\n          description: \u003e-\n            Time in epoch milliseconds when the storage configuration was\n            created.\n    CreateStorageConfigurationRequest:\n      type: object\n      required:\n        - storage_configuration_name\n        - root_bucket_info\n      properties:\n        storage_configuration_name:\n          type: string\n          description: The human-readable name of the storage configuration.\n          example: storage_conf_1\n          minLength: 4\n          maxLength: 256\n        root_bucket_info:\n          description: \"Root S3 bucket information.\"\n          type: object\n          properties:\n            bucket_name:\n              type: string\n              description: The name of the S3 bucket.\n              example: test-s3-bucket\n    ListStorageConfigurationsResponse:\n      description: \"Storage configuration objects.\"\n      type: array\n      items:\n        $ref: '#/components/schemas/StorageConfiguration'\n    Network:\n      type: object\n      properties:\n        network_id:\n          type: string\n          format: uuid\n          description: The Databricks network configuration ID.\n        network_name:\n          type: string\n          description: The human-readable name of the network configuration.\n          minLength: 4\n          maxLength: 256\n        vpc_id:\n          type: string\n          description: \u003e-\n            The ID of the VPC associated with this network configuration. VPC IDs can be used\n            in multiple networks.\n        subnet_ids:\n          type: array\n          items:\n            type: string\n            description: \u003e-\n              The ID of a subnet associated with this network configuration. Subnet IDs **cannot** be\n              used in multiple network configurations.\n          minLength: 2\n        security_group_ids:\n          type: array\n          items:\n            type: string\n            description:\n              \"ID of a security group associated with this network configuration. Security group\n              IDs **cannot** be used in multiple networks.\"\n          minLength: 1\n          maxLength: 5\n        vpc_status:\n          type: string\n          enum:\n            - UNATTACHED\n            - VALID\n            - BROKEN\n            - WARNED\n          description:\n            \"The status of this network configuration object in terms of its use in a\n            workspace:\\n\n\n            * `UNATTACHED`: Unattached.\\n\\n\n            * `VALID`: Valid.\\n\\n\n            * `BROKEN`: Broken.\\n\\n\n            * `WARNED`: Warned.\"\n        warning_messages:\n          description: Array of warning messages about the network configuration.\n          type: array\n          items:\n            $ref: '#/components/schemas/NetworkWarning'\n        error_messages:\n          description: Array of error messages about the network configuration.\n          type: array\n          items:\n            $ref: '#/components/schemas/NetworkHealth'\n        workspace_id:\n          type: integer\n          format: int64\n          description: Workspace ID associated with this network configuration.\n        account_id:\n          type: string\n          format: uuid\n          description:\n            \"The Databricks account ID associated with this network configuration.\"\n        creation_time:\n          type: integer\n          format: int64\n          description: Time in epoch milliseconds when the network was created.\n        vpc_endpoints:\n          $ref: '#/components/schemas/NetworkVpcEndpoints'\n    NetworkWarning:\n      type: object\n      properties:\n        warning_type:\n          type: string\n          enum:\n            - subnet\n            - securityGroup\n          description: \u003e-\n            The AWS resource associated with this warning: a subnet or a security group.\n        warning_message:\n          type: string\n          description: \u003e-\n            Details of the warning.\n    NetworkHealth:\n      type: object\n      properties:\n        error_type:\n          type: string\n          enum:\n            - credentials\n            - vpc\n            - subnet\n            - securityGroup\n            - networkAcl\n          description:\n            \"The AWS resource associated with this error: credentials, VPC, subnet,\n            security group, or network ACL.\"\n        error_message:\n          type: string\n          description: Details of the error.\n    NetworkVpcEndpoints:\n      type: object\n      description:\n        \"If specified, contains the VPC endpoints used to allow cluster communication from this VPC over [AWS PrivateLink](https://aws.amazon.com/privatelink/).\"\n      required:\n        - rest_api\n        - dataplane_relay\n      properties:\n        rest_api:\n          items:\n            type: string\n            format: uuid\n          description:\n            \"The VPC endpoint ID used by this Network to access the Databricks REST API. Databricks clusters make calls to our REST API as part of cluster creation, mlflow tracking, and many other features. Thus, this is required even if your workspace allows public access to the REST API.\\n\n\n            This is a list type for future compatibility, but currently only one VPC endpoint ID should be supplied.\\n\n\n            Note: This is the Databricks-specific ID of the VPC endpoint object in the Account API, not the AWS VPC endpoint ID that you see for your endpoint in the AWS Console.\"\n        dataplane_relay:\n          items:\n            type: string\n            format: uuid\n          description:\n            \"The VPC endpoint ID used by this Network to access the Databricks secure cluster connectivity relay. See [Secure Cluster Connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html).\\n\n\n            This is a list type for future compatibility, but currently only one VPC endpoint ID should be\n            supplied.\\n\n\n            Note: This is the Databricks-specific ID of the VPC endpoint object in the Account API, not the AWS VPC endpoint ID that you see for your endpoint in the AWS Console.\"\n    CreateNetworkRequest:\n      type: object\n      required:\n        - network_name\n        - vpc_id\n        - subnet_ids\n        - security_group_ids\n      properties:\n        network_name:\n          type: string\n          description: The human-readable name of the network configuration.\n          minLength: 4\n          maxLength: 256\n        vpc_id:\n          type: string\n          description: \u003e-\n            The ID of the VPC associated with this network. VPC IDs can be used\n            in multiple network configurations.\n        subnet_ids:\n          description:\n            \"IDs of at least 2 subnets associated with this network. Subnet IDs **cannot** be\n            used in multiple network configurations.\"\n          type: array\n          items:\n            type: string\n            description:\n              \"ID of subnet associated with this network. Subnet IDs **cannot** be\n              used in multiple network configurations.\"\n          minLength: 2\n        security_group_ids:\n          description:\n            \"IDs of 1 to 5 security groups associated with this network. Security groups\n            IDs **cannot** be used in multiple network configurations.\"\n          type: array\n          items:\n            type: string\n            description:\n              \"ID of security group associated with this network. Security group\n              IDs *cannot** be used in multiple network configurations.\"\n          minLength: 1\n          maxLength: 5\n        vpc_endpoints:\n          $ref: '#/components/schemas/NetworkVpcEndpoints'\n    ListNetworksResponse:\n      description: Array of network configuration objects.\n      type: array\n      items:\n        $ref: '#/components/schemas/Network'\n    CustomerManagedKey:\n      type: object\n      properties:\n        customer_managed_key_id:\n          type: string\n          format: uuid\n          description: ID of the encryption key configuration object.\n        account_id:\n          type: string\n          format: uuid\n          description:\n            \"The Databricks account ID that holds the customer-managed key.\"\n        aws_key_info:\n          $ref: '#/components/schemas/AwsKeyInfo'\n        creation_time:\n          type: integer\n          format: int64\n          description: Time in epoch milliseconds when the customer key was created.\n        use_cases:\n            type: array\n            items: string\n            description:\n              \"The cases that the key can be used for. Include one or both of these options:\\n\n                * `MANAGED_SERVICES`: Encrypts notebook and secret data in the control plane\\n\n                * `STORAGE`: Encrypts the workspace's root S3 bucket (root DBFS and system data) and optionally cluster EBS volumes.\"\n            example: [MANAGED_SERVICES, STORAGE]\n    CreateAwsKeyInfo:\n      type: object\n      required:\n        - key_arn\n      properties:\n        key_arn:\n          type: string\n          description: The AWS KMS key's Amazon Resource Name (ARN). Note that the key's AWS region is inferred from the ARN.\n          example: \u003e-\n            arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\n        key_alias:\n          type: string\n          description: The AWS KMS key alias.\n          example: alias/projectKey1\n        reuse_key_for_cluster_volumes:\n          type: boolean\n          description: This field applies only if the `use_cases` property includes `STORAGE`. If this is set to `true` or omitted, the key is also used to encrypt cluster EBS volumes. To not use this key also for encrypting EBS volumes, set this to `false`.\n          example: true\n    AwsKeyInfo:\n      type: object\n      required:\n        - key_arn\n        - key_region\n      properties:\n        key_arn:\n          type: string\n          description: The AWS KMS key's Amazon Resource Name (ARN).\n          example: \u003e-\n            arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321\n        key_alias:\n          type: string\n          description: The AWS KMS key alias.\n          example: alias/projectKey1\n        key_region:\n          type: string\n          description: The AWS KMS key region.\n          example: us-east-1\n        reuse_key_for_cluster_volumes:\n          type: boolean\n          description: This field applies only if the `use_cases` property includes `STORAGE`. If this is set to `true` or omitted, the key is also used to encrypt cluster EBS volumes. If you do not want to use this key for encrypting EBS volumes, set to `false`..\n          example: true\n    CreateCustomerManagedKeyRequest:\n      type: object\n      required:\n        - aws_key_info\n        - use_cases\n      properties:\n        aws_key_info:\n          $ref: '#/components/schemas/CreateAwsKeyInfo'\n        use_cases:\n          type: array\n          items: string\n          description:\n            \"The cases that the key can be used for. Include one or both of these options:\\n\n                * `MANAGED_SERVICES`: Encrypts notebook and secret data in the control plane\\n\n                * `STORAGE`: Encrypts the workspace's root S3 bucket (root DBFS and system data) and optionally cluster EBS volumes.\"\n          example: [MANAGED_SERVICES, STORAGE]\n    ListCustomerManagedKeysResponse:\n      description: Array of key configuration objects.\n      type: array\n      items:\n        $ref: '#/components/schemas/CustomerManagedKey'\n    WorkspaceEncryptionKeyRecord:\n      type: object\n      properties:\n        record_id:\n          type: string\n          format: uuid\n          description: ID for the workspace-key mapping record\n        workspace_id:\n          type: integer\n          format: int64\n          description: Workspace ID.\n        customer_managed_key_id:\n          type: string\n          format: uuid\n          description: \u003e-\n            ID of the encryption key configuration object.\n        update_time:\n          type: integer\n          format: int64\n          description: Time in epoch milliseconds when the record was added.\n        key_status:\n          type: string\n          enum:\n            - UNKNOWN\n            - ATTACHED\n            - DETACHED\n          example: ATTACHED\n        aws_key_info:\n          $ref: '#/components/schemas/AwsKeyInfo'\n        use_case:\n          type: string\n          enum:\n            - MANAGED_SERVICES\n            - STORAGE\n          description:\n            \"Possible values are:\\n\n             - `MANAGED_SERVICES`: Encrypts notebook and secret data in the control plane\\n\n             - `STORAGE`: Encrypts the workspace's root S3 bucket (root DBFS and system data) and optionally cluster EBS volumes.\"\n          example: STORAGE\n    ListWorkspaceEncryptionKeyRecordsResponse:\n      type: object\n      properties:\n        workspaceEncryptionKeyRecords:\n          type: array\n          items:\n            $ref: '#/components/schemas/WorkspaceEncryptionKeyRecord'\n    Workspace:\n      type: object\n      properties:\n        workspace_id:\n          type: integer\n          format: int64\n          description: Workspace ID.\n        workspace_name:\n          type: string\n          description: The human-readable name of the workspace.\n          minLength: 1\n          maxLength: 100\n        deployment_name:\n          type: string\n          description:\n            \"The deployment name defines part of the subdomain for the workspace.\n            The workspace URL for web application and REST APIs is `\u003cdeployment-name\u003e.cloud.databricks.com`.\\n\n\n            This value must be unique across all non-deleted deployments across all AWS regions.\"\n          maxLength: 64\n        aws_region:\n          type: string\n          description: \u003e-\n            The AWS region of the workspace Data Plane. For example, `us-west-2`.\n        credentials_id:\n          type: string\n          format: uuid\n          description: ID of the workspace's credential configuration object.\n        storage_configuration_id:\n          type: string\n          format: uuid\n          description: ID of the workspace's storage configuration object.\n        account_id:\n          type: string\n          format: uuid\n          description: Databricks account ID\n        workspace_status:\n          type: string\n          enum:\n            - NOT_PROVISIONED\n            - PROVISIONING\n            - RUNNING\n            - FAILED\n            - BANNED\n            - CANCELLING\n          description:\n            \"The status of the workspace.\n\n            For workspace creation, it is typically initially `PROVISIONING`. Continue to\n            check the status until the status is `RUNNING`. For detailed instructions of\n            creating a new workspace with this API **including error handling** see\n            [Create a new workspace using the Account Management\n            API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\"\n          example: RUNNING\n        workspace_status_message:\n          type: string\n          description: Message describing the current workspace status.\n          example: Workspace resources are being set up.\n        managed_services_customer_managed_key_id:\n          type: string\n          format: uuid\n          description: ID of the key configuration for encrypting managed services.\n        private_access_settings_id:\n          type: string\n          format: uuid\n          description:\n            \"Only used for PrivateLink, which is in Public Preview. This is the ID of the workspace's private access settings object. This ID must be specified for customers using [AWS PrivateLink](https://aws.amazon.com/privatelink/) for either front-end (user-to-workspace connection), back-end (data plane to control plane connection), or both connection types.\\n\n            \n            Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\"\n        creation_time:\n          type: integer\n          format: int64\n          description: Time in epoch milliseconds when the workspace was created.\n        pricing_tier:\n          type: string\n          enum:\n            - UNKNOWN\n            - COMMUNITY_EDITION\n            - STANDARD\n            - PREMIUM\n            - ENTERPRISE\n            - DEDICATED\n          description:\n            \"The pricing tier of the workspace.\n\n            See https://databricks.com/product/aws-pricing for available pricing tier information.\"\n          example: PREMIUM\n        storage_customer_managed_key_id:\n          type: string\n          format: uuid\n          description: ID of the key configuration for encrypting workspace storage.\n    CreateWorkspaceRequest:\n      type: object\n      required:\n        - workspace_name\n        - aws_region\n        - credentials_id\n        - storage_configuration_id\n      properties:\n        workspace_name:\n          type: string\n          description: \u003e-\n            The workspace's human-readable name.\n          example: My workspace 1\n        deployment_name:\n          type: string\n          pattern: '^(([a-z0-9][a-z0-9-]*[a-z0-9])|([a-z0-9]))|(EMPTY)$'\n          description:\n            \"The deployment name defines part of the subdomain for the workspace.\n            The workspace URL for web application and REST APIs is `\u003cworkspace-deployment-name\u003e.cloud.databricks.com`.\n            For example, if the deployment name is `abcsales`, your workspace URL will be `https://abcsales.cloud.databricks.com`.\n            Hyphens are allowed.  This property supports only the set of characters that are allowed in a subdomain.\\n\n\n            If your account has a non-empty deployment name prefix at workspace creation time,\n            the workspace deployment name changes so that the beginning has the account prefix and a hyphen.\n            For example, if your account's deployment prefix is `acme` and the workspace deployment name is `workspace-1`,\n            the `deployment_name` field becomes `acme-workspace-1` and that is the value that will be returned in JSON responses for the `deployment_name` field.\n            The workspace URL is `acme-workspace-1.cloud.databricks.com`.\\n\n\n            If your account has a non-empty deployment name prefix and you set `deployment_name` to the reserved keyword `EMPTY`,\n            `deployment_name` is just the account prefix only. For example, if your account's deployment prefix is `acme`\n            and the workspace deployment name is `EMPTY`, `deployment_name` becomes `acme` only and the workspace URL is `acme.cloud.databricks.com`.\\n\n\n            Contact your Databricks representatives to add an account deployment name prefix to your account.\n            If you do not have a deployment name prefix, the special deployment name value `EMPTY` is invalid.\\n\n\n            This value must be unique across all non-deleted deployments across all AWS regions.\\n\n\n            If a new workspace omits this property, the server generates a unique deployment name for you with the pattern `dbc-xxxxxxxx-xxxx`.\"\n          example: workspace_1\n        aws_region:\n          type: string\n          description: The AWS region of the workspace's Data Plane.\n          example: us-west-2\n        credentials_id:\n          type: string\n          format: uuid\n          description: ID of the workspace's credential configuration object\n          example: ccc64f28-ebdc-4c89-add9-5dcb6d7727d8\n        storage_configuration_id:\n          type: string\n          format: uuid\n          description: The ID of the workspace's storage configuration object.\n          example: b43a6064-04c1-4e1c-88b6-d91e5b136b13\n        network_id:\n          type: string\n          format: uuid\n          description:\n            \"The ID of the workspace's network configuration object. To use [AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) (Public Preview), this field is required.\"\n          example: fd0cc5bc-683c-47e9-b15e-144d7744a496\n        managed_services_customer_managed_key_id:\n          type: string\n          format: uuid\n          description:\n            \"The ID of the workspace's managed services encryption key configuration object. This is used to encrypt the workspace's notebook and secret data in the control plane, as well as Databricks SQL queries and query history. The provided key configuration object property `use_cases` must contain `MANAGED_SERVICES`.\"\n          example: 849b3d6b-e68e-468d-b3e5-deb08b03c56d\n        private_access_settings_id:\n          type: string\n          format: uuid\n          description:\n            \"Only used for PrivateLink, which is in Public Preview. This is the ID of the workspace's private access settings object. This ID must be specified for customers using [AWS PrivateLink](https://aws.amazon.com/privatelink/) for either front-end (user-to-workspace connection), back-end (data plane to control plane connection), or both connection types.\\n\n            \n            Before configuring PrivateLink, it is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\"\n        pricing_tier:\n          type: string\n          enum:\n            - STANDARD\n            - PREMIUM\n            - ENTERPRISE\n          description:\n            \"The pricing tier of the workspace. If you do not provide this, the API will default to\n            the highest pricing tier available to your account.\n\n            See https://databricks.com/product/aws-pricing for available pricing tier information.\"\n          example: PREMIUM\n        storage_customer_managed_key_id:\n          type: string\n          format: uuid\n          description:\n            \"The ID of the workspace's storage encryption key configuration object. This is used to encrypt the workspace's root S3 bucket (root DBFS and system data) and optionally cluster EBS volumes. The provided key configuration object property `use_cases` must contain `STORAGE`.\"\n    UpdateWorkspaceRequest:\n      type: object\n      properties:\n        aws_region:\n          type: string\n          description: \u003e-\n            The AWS region of the workspace's Data Plane. For example, `us-west-2`. This parameter is available only for updating failed workspaces.\n          example: us-west-2\n        credentials_id:\n          type: string\n          format: uuid\n          description: ID of the workspace's credential configuration object. This parameter is available for updating both failed and running workspaces.\n        storage_configuration_id:\n          type: string\n          format: uuid\n          description: The ID of the workspace's storage configuration object. This parameter is available only for updating failed workspaces.\n        network_id:\n          type: string\n          format: uuid\n          description: \n            \"The ID of the workspace's network configuration object. Used only if you already use a customer-managed VPC. This change is supported only if you specified a network configuration ID when the workspace was created. In other words, you cannot switch from a Databricks-managed VPC to a customer-managed VPC. This parameter is available for updating both failed and running workspaces. Note: You cannot use a network configuration update in this API to add support for PrivateLink (in Public Preview). To add PrivateLink to an existing workspace, contact your Databricks representative.\"\n        managed_services_customer_managed_key_id:\n          type: string\n          format: uuid\n          description: The ID of the workspace's managed services encryption key configuration object. This parameter is available only for updating failed workspaces.\n        storage_customer_managed_key_id:\n          type: string\n          format: uuid\n          description: The ID of the key configuration object for workspace storage. This parameter is available for updating both failed and running workspaces.\n    ListWorkspacesResponse:\n      description: An array of workspaces.\n      type: array\n      items:\n        $ref: '#/components/schemas/Workspace'\n    WorkspaceId:\n      type: integer\n      format: int64\n    LogDeliveryConfigStatus:\n      type: string\n      enum:\n        - ENABLED\n        - DISABLED\n      description: Status of log delivery configuration. Set to `ENABLED` (enabled) or `DISABLED` (disabled). Defaults to `ENABLED`. You can [enable or disable the configuration](#operation/patch-log-delivery-config-status) later. Deletion of a configuration is not supported, so disable a log delivery configuration that is no longer needed.\n    CreateLogDeliveryConfigurationParams:\n      type: object\n      properties:\n        config_name:\n          type: string\n          minLength: 0\n          maxLength: 255\n          description: The optional human-readable name of the log delivery configuration. Defaults to empty.\n        status:\n          $ref: '#/components/schemas/LogDeliveryConfigStatus'\n        log_type:\n          type: string\n          enum:\n            - BILLABLE_USAGE\n            - AUDIT_LOGS\n          description:\n            \"Log delivery type. Supported values are:\\n\n            \n            * `BILLABLE_USAGE` — Configure [billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html). For the CSV schema, see the [View billable usage](https://docs.databricks.com/administration-guide/account-settings/usage.html).\\n\n\n            * `AUDIT_LOGS` — Configure [audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html). For the JSON schema, see [Configure audit logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html)\"\n\n        output_format:\n          type: string\n          enum:\n            - CSV\n            - JSON\n          description:\n            \"The file type of log delivery.\\n\n            \n            *  If `log_type` is `BILLABLE_USAGE`, this value must be `CSV`. Only the CSV (comma-separated values) format is supported. For the schema, see the [View billable usage](https://docs.databricks.com/administration-guide/account-settings/usage.html)\n\n            * If `log_type` is `AUDIT_LOGS`, this value must be `JSON`. Only the JSON (JavaScript Object Notation) format is supported. For the schema, see the [Configuring audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\"\n        credentials_id:\n          type: string\n          format: uuid\n          description:\n            \"The ID for a [Databricks credential configuration](#operation/create-credential-config) that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page. See [Configure billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\"\n        storage_configuration_id:\n          type: string\n          format: uuid\n          description: \u003e-\n            \"The ID for a [Databricks storage configuration](#operation/create-storage-config)  that represents the S3 bucket with bucket policy as described in the main billable usage documentation page. See [Configure billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\"\n        workspace_ids_filter:\n          type: array\n          items:\n            $ref: '#/components/schemas/WorkspaceId'\n          description:\n            \"Optional filter of workspace IDs to deliver logs for. By default the workspace filter is empty and log delivery applies at the account level, delivering workspace level logs for all workspaces in your account, plus account level logs. You can optionally set this field to an array of workspace IDs (each one is an `int64`) to which log delivery should apply to, in which case only workspace level logs relating to the specified workspaces will be delivered.\n\n            If you plan to use different log delivery configurations for different workspaces, set this field explicitly. Be aware that delivery configurations mentioning specific workspaces won't apply to new workspaces created in the future, and delivery won't include account level logs.\n\n            For some types of Databricks deployments there is only one workspace per account ID, so this field is unnecessary.\"\n        delivery_path_prefix:\n          type: string\n          description: The optional delivery path prefix within AWS S3 storage. Defaults to empty, which means that logs are delivered to the root of the bucket. This must be a valid S3 object key. This must not start or end with a slash character.\n        delivery_start_time:\n          type: string\n          pattern: \"2[0-9][0-9][0-9]-(0[1-9]|1[012])\"\n          description: This field applies only if `log_type` is `BILLABLE_USAGE`. This is the optional start month and year for delivery, specified in `YYYY-MM` format. Defaults to current year and month.  `BILLABLE_USAGE` logs are not available for usage before March 2019 (`2019-03`).\n    LogDeliveryConfiguration:\n      allOf:\n        - $ref: '#/components/schemas/CreateLogDeliveryConfigurationParams'\n      type: object\n      properties:\n        account_id:\n          type: string\n          format: uuid\n          description: \u003e-\n            The Databricks account ID that hosts the log delivery configuration.\n        config_id:\n          type: string\n          format: uuid\n          description: Databricks log delivery configuration ID.\n        creation_time:\n          type: integer\n          format: int64\n          description: Time in epoch milliseconds when the log delivery configuration was created.\n        update_time:\n          type: integer\n          format: int64\n          description: Time in epoch milliseconds when the log delivery configuration was updated.\n        log_delivery_status:\n          type: object\n          description: Databricks log delivery status.\n          properties:\n            status:\n              type: string\n              enum:\n                - CREATED\n                - SUCCEEDED\n                - USER_FAILURE\n                - SYSTEM_FAILURE\n                - NOT_FOUND\n              description:\n                \"The status string for log delivery. Possible values are:\n                * `CREATED`: There were no log delivery attempts since the config was created.\n                * `SUCCEEDED`: The latest attempt of log delivery has succeeded completely.\n                * `USER_FAILURE`: The latest attempt of log delivery failed because of misconfiguration of customer provided permissions on role or storage.\n                * `SYSTEM_FAILURE`: The latest attempt of log delivery failed because of an Databricks internal error. Contact support if it doesn't go away soon.\n                * `NOT_FOUND`: The log delivery status as the configuration has been disabled since the release of this feature or there are no workspaces in the account.\"\n            message:\n              type: string\n              description: Informative message about the latest log delivery attempt. If the log delivery fails with USER_FAILURE, error details will be provided for fixing misconfigurations in cloud permissions.\n            last_attempt_time:\n              type: string\n              format: date-time\n              description: The UTC time for the latest log delivery attempt.\n            last_successful_attempt_time:\n              type: string\n              format: date-time\n              description: The UTC time for the latest successful log delivery.\n    WrappedLogDeliveryConfiguration:\n      type: object\n      properties:\n        log_delivery_configuration:\n          allOf:\n            - $ref: '#/components/schemas/LogDeliveryConfiguration'\n    WrappedLogDeliveryConfigurations:\n      type: object\n      properties:\n        log_delivery_configurations:\n          type: array\n          items:\n            $ref: '#/components/schemas/LogDeliveryConfiguration'\n    WrappedCreateLogDeliveryConfiguration:\n      type: object\n      properties:\n        log_delivery_configuration:\n          allOf:\n            - $ref: '#/components/schemas/CreateLogDeliveryConfigurationParams'\n          required:\n            - log_type\n            - output_format\n            - credentials_id\n            - storage_configuration_id\n    UpdateLogDeliveryConfigurationStatusRequest:\n      type: object\n      required:\n        - status\n      properties:\n        status:\n          $ref: '#/components/schemas/LogDeliveryConfigStatus'\n    Error:\n      type: object\n      properties:\n        message:\n          type: string\n          description: Cause of the error\n    PrivateAccessSettings:\n      type: object\n      properties:\n        private_access_settings_id:\n          type: string\n          format: uuid\n          description: Databricks private access settings ID.\n        private_access_settings_name:\n          type: string\n          description: The human-readable name of the private access settings object.\n          minLength: 4\n          maxLength: 256\n        public_access_enabled:\n          type: boolean\n          description:\n            \"Determines if the workspace can be accessed over public internet. For fully private workspaces, you can optionally specify `false`, but only if you implement both the front-end and the back-end PrivateLink connections. Otherwise, specify `true`, which means that public access is still enabled.\"\n        region:\n          type: string\n          description: The AWS region for workspaces attached to this private access settings object.\n        account_id:\n          type: string\n          format: uuid\n          description:\n            \"The Databricks account ID that hosts the credential.\"\n        private_access_level:\n          type: string\n          enum:\n            - ANY\n            - ACCOUNT\n            - ENDPOINT\n          example: \"ENDPOINT\"\n          description:\n            \"The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches\n            this private access settings object.\n\n            * `ANY` (the default): Any VPC endpoint can connect to your workspace.\n\n            * `ACCOUNT` level access lets only VPC endpoints that are registered in your Databricks account connect to your workspace.\n\n            * `ENDPOINT` level access lets only specified VPC endpoints connect to your workspace. See the `allowed_vpc_endpoint_ids`\n             for details.\"\n        allowed_vpc_endpoint_ids:\n          type: array\n          items:\n            type: string\n            format: uuid\n          description:\n            \"An array of Databricks VPC endpoint IDs. This is the Databricks ID returned when registering the VPC endpoint configuration in\n            your Databricks account. This is _not_ the ID of the VPC endpoint in AWS.\n\n\n            Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of VPC endpoints registered in\n            your Databricks account that can connect to your workspace over AWS PrivateLink.\n\n\n            Note: if hybrid access to your workspace is enabled by setting `public_access_enabled` to `true`, then this\n            control only works for PrivateLink connections. To control how your workspace is accessed via public internet,\n            see the article on [IP access lists](https://docs.databricks.com/security/network/ip-access-list.html).\"\n    UpsertPrivateAccessSettingsRequest:\n      type: object\n      required:\n        - private_access_settings_name\n        - region\n      properties:\n        private_access_settings_name:\n          type: string\n          description: The human-readable name of the private access settings object.\n          minLength: 4\n          maxLength: 256\n        region:\n          type: string\n          description: The AWS region for workspaces associated with this private access settings object. This must be a [region that Databricks supports for PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html). \n        public_access_enabled:\n          description:\n            \"Determines if the workspace can be accessed over public internet. For fully private workspaces, you can optionally specify `false`, but only if you implement both the front-end and the back-end PrivateLink connections. Otherwise, specify `true`, which means that public access is still enabled.\"\n          type: boolean\n          default: false\n        private_access_level:\n          type: string\n          enum:\n            - ANY\n            - ACCOUNT\n            - ENDPOINT\n          example: \"ENDPOINT\"\n          default: ANY\n          description:\n            \"The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches\n            this private access settings object.\n\n            * `ANY` level access lets any VPC endpoint connect to your workspace.\n\n            * `ACCOUNT` level access lets only VPC endpoints that are registered in your Databricks account connect to your workspace.\n\n            * `ENDPOINT` level access lets only specified VPC endpoints connect to your workspace. Please see the `allowed_vpc_endpoint_ids`\n            documentation for more details.\"\n        allowed_vpc_endpoint_ids:\n          type: array\n          items:\n            type: string\n            format: uuid\n          description:\n            \"An array of Databricks VPC endpoint IDs. This is the Databricks ID that is returned when registering the VPC endpoint configuration in\n            your Databricks account. This is not the ID of the VPC endpoint in AWS.\n\n\n            Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of VPC endpoints that in\n            your account that can connect to your workspace over AWS PrivateLink.\n\n\n            If hybrid access to your workspace is enabled by setting `public_access_enabled` to `true`, then this\n            control only works for PrivateLink connections. To control how your workspace is accessed via public internet,\n            see the article for [IP access lists](https://docs.databricks.com/security/network/ip-access-list.html).\"\n    ListPrivateAccessSettingsResponse:\n      description: \"Private access settings objects.\"\n      type: array\n      items:\n        $ref: '#/components/schemas/PrivateAccessSettings'\n    VPCEndpoint:\n      type: object\n      properties:\n        vpc_endpoint_id:\n          type: string\n          format: uuid\n          description:\n            \"Databricks VPC endpoint ID. This is the Databricks-specific name of the VPC endpoint. Do not confuse this with the `aws_vpc_endpoint_id`, which is the ID within AWS of the VPC endpoint.\"\n        vpc_endpoint_name:\n          type: string\n          description: The human-readable name of the storage configuration.\n          minLength: 4\n          maxLength: 256\n        aws_vpc_endpoint_id:\n          description:\n            \"The ID of the VPC endpoint object in AWS.\"\n          type: string\n        aws_endpoint_service_id:\n          type: string\n          description:\n            \"The ID of the Databricks [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html) that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks\n            PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\"\n        use_case:\n          type: string\n          enum:\n            - WORKSPACE_ACCESS\n            - DATAPLANE_RELAY_ACCESS\n          description:\n            \"This enumeration represents the type of Databricks VPC [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html) that was used when creating this VPC endpoint.\\n\n            \n            If the VPC endpoint connects to the Databricks control plane for either the front-end connection or the back-end REST API connection, the value is `WORKSPACE_ACCESS`.\\n\n            \n            If the VPC endpoint connects to the Databricks workspace for the back-end [secure cluster connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html) relay, the value is `DATAPLANE_RELAY_ACCESS`.\"\n        region:\n          type: string\n          description: The AWS region in which this VPC endpoint object exists.\n        account_id:\n          type: string\n          format: uuid\n          description:\n            \"The Databricks account ID that hosts the VPC endpoint configuration.\"\n        aws_account_id:\n          type: string\n          description: The AWS Account in which the VPC endpoint object exists.\n        state:\n          type: string\n          description:\n            \"The current state (such as `available` or `rejected`) of the VPC endpoint. Derived from AWS. For the full set of values, see [AWS DescribeVpcEndpoint documentation](https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-vpc-endpoints.html)\n            for details.\"\n    CreateVPCEndpointRequest:\n      type: object\n      required:\n        - vpc_endpoint_name\n        - aws_vpc_endpoint_id\n        - region\n      properties:\n        vpc_endpoint_name:\n          type: string\n          description: The human-readable name of the storage configuration.\n          minLength: 4\n          maxLength: 256\n        aws_vpc_endpoint_id:\n          description:\n            \"The ID of the VPC endpoint object in AWS.\"\n          type: string\n        region:\n          type: string\n          description: The AWS region in which this VPC endpoint object exists\n    ListVPCEndpointsResponse:\n      description: \"List VPC endpoint configurations.\"\n      type: array\n      items:\n        $ref: '#/components/schemas/VPCEndpoint'\n    UsageDownloadMonth:\n      type: string\n      pattern: \"2[0-9][0-9][0-9]-(0[1-9]|1[012])\"\n      description: Format specification for month in the format `YYYY-MM`. This is used to specify billable usage `start_month` and `end_month` properties. Note that billable usage logs are unavailable before March 2019 (`2019-03`).\n    BudgetAlert:\n      type: object\n      properties:\n        min_percentage:\n          description: Percentage of the target amount used in the currect period that will trigger a notification\n          type: integer\n          minimum: 1\n          maximum: 100000\n        email_notifications:\n          description: List of email addresses to be notified when budget percentage is exceeded in the given period\n          type: array\n          items:\n            type: string\n            example: foo@bar.com\n    BudgetFilter:\n      type: string\n      description:\n        \"\n        SQL-like filter expression with workspaceId, SKU and tag. Usage in your account that matches this expression will be counted in this budget.\\n\n\n        Supported properties on left-hand side of comparison:\\n\n        * `workspaceId` - the ID of the workspace\\n\n        * `sku` - SKU of the cluster, e.g. `STANDARD_ALL_PURPOSE_COMPUTE` \\n\n        * `tag.tagName`, `tag.'tag name'` - tag of the cluster \\n\n\n        Supported comparison operators:\\n\n        * `=` - equal \\n\n        * `!=` - not equal \\n\n\n        Supported logical operators: `AND`, `OR`.\\n\n\n        Examples:\\n\n        * `workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my tag'='my value')`\\n\n        * `workspaceId!=456`\\n\n        * `sku='STANDARD_ALL_PURPOSE_COMPUTE' OR sku='PREMIUM_ALL_PURPOSE_COMPUTE'`\\n\n        * `tag.name1='value1' AND tag.name2='value2'`\\n\n        \"\n      example: \"workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my tag'='my value')\"\n    BudgetPeriod:\n      type: string\n      description:\n        \"\n        Period length in years, months, weeks and/or days.\\n\n        Examples: `1 month`, `30 days`, `1 year, 2 months, 1 week, 2 days`\\n\n        \"\n      example: \"1 month\"\n    BudgetCreateRequest:\n      description: Budget configuration to be created\n      type: object\n      required:\n        - name\n        - period\n        - start_date\n        - target_amount\n        - filter\n      properties:\n        name:\n          type: string\n          description: Human-readable name of the budget\n        period:\n          $ref: '#/components/schemas/BudgetPeriod'\n        start_date:\n          type: string\n          format: date\n          description: Start date of the budget period calculation\n        end_date:\n          type: string\n          format: date\n          description: Optional end date of the budget\n        target_amount:\n          type: string\n          description: Target amount of the budget per period in USD\n          example: \"1234.56\"\n        filter:\n          $ref: '#/components/schemas/BudgetFilter'\n        alerts:\n          type: array\n          items:\n            $ref: '#/components/schemas/BudgetAlert'\n    BudgetWithStatus:\n      description: Budget configuration with daily status\n      type: object\n      properties:\n        budget_id:\n          type: string\n          format: uuid\n        name:\n          type: string\n          description: Human-readable name of the budget\n        period:\n          $ref: '#/components/schemas/BudgetPeriod'\n        start_date:\n          type: string\n          format: date\n          description: Start date of the budget period calculation\n        end_date:\n          type: string\n          format: date\n          description: Optional end date of the budget\n        target_amount:\n          type: string\n          description: Target amount of the budget per period in USD\n          example: \"1234.56\"\n        filter:\n          $ref: '#/components/schemas/BudgetFilter'\n        alerts:\n          type: array\n          items:\n            $ref: '#/components/schemas/BudgetAlert'\n        status_daily:\n          description: Amount used in the budget for each day (non-cumulative)\n          type: array\n          items:\n            type: object\n            properties:\n              date:\n                type: string\n                format: date\n              amount:\n                description: Amount used in this day in USD\n                type: string\n                example: \"123.45\"\n        creation_time:\n          type: string\n          format: date-time\n        update_time:\n          type: string\n          format: date-time\n    BudgetList:\n      description: \"List of Budgets\"\n      type: object\n      properties:\n        budgets:\n          type: array\n          items:\n            $ref: '#/components/schemas/BudgetWithStatus'\n  responses:\n    BadRequest:\n      description: The request is malformed.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n    Unauthenticated:\n      description: The request is unauthenticated. The user's credentials are missing or incorrect.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n    Forbidden:\n      description: The request is forbidden from being fulfilled.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n    NotFound:\n      description: The requested resource does not exist.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n    Conflict:\n      description: The request conflicts with the current state of the target resource.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n    InternalError:\n      description: The request is not handled correctly due to a server error.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n    ServiceUnavailable:\n      description: The service is unavailable.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n  securitySchemes:\n    basicAuth:\n      type: http\n      scheme: basic\nsecurity:\n  - basicAuth: []\n",
			"canonicalURL": "/github.com/gbrueckl/Databricks.API.PowerShell@d1d54858d69d4f5c82f2ce110e01070a11ea05bd/-/blob/Docs/OpenAPISpecs/account-2.0-aws.yaml",
			"externalURLs": [
				{
					"url": "https://github.com/gbrueckl/Databricks.API.PowerShell/blob/d1d54858d69d4f5c82f2ce110e01070a11ea05bd/Docs/OpenAPISpecs/account-2.0-aws.yaml",
					"serviceKind": "GITHUB"
				}
			]
		}
	},
	"Error": "unmarshal: json: line 1:351: callback: json: cannot unmarshal string into Go struct field Schema.items of type ogen.Schema"
}