{
	"File": {
		"__typename": "FileMatch",
		"repository": {
			"name": "github.com/microsoft/openpai-protocol"
		},
		"file": {
			"name": "schema.yaml",
			"size": 0,
			"path": "schemas/v2/schema.yaml",
			"byteSize": 20790,
			"content": "openapi: 3.0.1\ninfo:\n  title: OpenPAI Protocol Specification\n  description: |\n    [OpenPAI](https://github.com/microsoft/pai) Protocol specification.\n  version: v2.0.0-alpha\nexternalDocs:\n  description: Find out more about OpenPAI Protocol.\n  url: https://github.com/microsoft/openpai-protocol\npaths: {}\ncomponents:\n  schemas:\n    PrerequisiteInfo:\n      type: object\n      properties:\n        protocolVersion:\n          oneOf:\n            - type: number\n              enum:\n                - 2\n            - type: string\n              enum:\n                - \"2\"\n          description: Protocol version, current version is 2.\n          example: 2\n        name:\n          type: string\n          description: String in ^[a-zA-Z0-9_-]+$ format, no longer than 255 characters.\n          pattern: \"^[a-zA-Z0-9_-]+$\"\n          minLength: 1\n          maxLength: 255\n          example: tensorflow_distributed_training\n        version:\n          type: string\n          description: Component version, default is latest.\n          example: v1.0\n        contributor:\n          type: string\n          example: OpenPAI\n        description:\n          type: string\n          example: Here is the description.\n    DockerImagePrerequisite:\n      allOf:\n        - $ref: \"#/components/schemas/PrerequisiteInfo\"\n        - type: object\n          properties:\n            type:\n              type: string\n              description: Component type.\n              enum:\n                - dockerimage\n            auth:\n              type: object\n              description: Only available when the type is dockerimage.\n              properties:\n                username:\n                  type: string\n                password:\n                  type: string\n                  description: If a password is needed, it should be referenced as a secret.\n                registryuri:\n                  type: string\n            uri:\n              type: string\n          required:\n            - name\n            - type\n            - uri\n    ScriptOrOutputPrerequisite:\n      allOf:\n        - $ref: \"#/components/schemas/PrerequisiteInfo\"\n        - type: object\n          properties:\n            type:\n              type: string\n              description: Component type.\n              enum:\n                - script\n                - output\n            plugin:\n              type: string\n              description: It indicates which plugin will be used to parse this prerequisite.\n            require:\n              type: array\n              description: It indicates the required prerequisites to be imported to use this prerequisite.\n              items:\n                type: string\n            uri:\n              type: string\n          additionalProperties: true\n          required:\n            - name\n            - type\n    DataPrerequisite:\n      allOf:\n        - $ref: \"#/components/schemas/PrerequisiteInfo\"\n        - type: object\n          properties:\n            type:\n              type: string\n              description: Component type.\n              enum:\n                - data\n            plugin:\n              type: string\n              description: It indicates which plugin will be used to parse this prerequisite.\n            require:\n              type: array\n              description: It indicates the required prerequisites to be imported to use this prerequisite.\n              items:\n                type: string\n            uri:\n              type: array\n              items:\n                type: string\n              minItems: 1\n          additionalProperties: true\n          required:\n            - type\n    JobProtocol:\n      allOf:\n        - $ref: \"#/components/schemas/PrerequisiteInfo\"\n        - type: object\n          description: Job protocol specification.\n          properties:\n            type:\n              type: string\n              description: Component type, should be \"job\" here.\n              enum:\n                - job\n              example: job\n            prerequisites:\n              type: array\n              description: Each item is the protocol for data, script, dockerimage, or output type.\n              items:\n                oneOf:\n                  - $ref: \"#/components/schemas/DockerImagePrerequisite\"\n                  - $ref: \"#/components/schemas/ScriptOrOutputPrerequisite\"\n                  - $ref: \"#/components/schemas/DataPrerequisite\"\n              minItems: 1\n              example:\n                - protocolVersion: 2\n                  name: tf_example\n                  type: dockerimage\n                  version: latest\n                  contributor: Alice\n                  description: python3.5, tensorflow\n                  auth:\n                    username: user\n                    password: \u003c% $secrets.docker_password %\u003e\n                    registryuri: openpai.azurecr.io\n                  uri: openpai/pai.example.tensorflow\n                - protocolVersion: 2\n                  name: tensorflow_cifar10_model\n                  type: output\n                  version: latest\n                  contributor: Alice\n                  description: cifar10 data output\n                  uri: hdfs://10.151.40.179:9000/core/cifar10_model\n                - protocolVersion: 2\n                  name: tensorflow_cnnbenchmarks\n                  type: script\n                  version: 84820935288cab696c9c2ac409cbd46a1f24723d\n                  contributor: MaggieQi\n                  description: tensorflow benchmarks\n                  uri: github.com/MaggieQi/benchmarks\n                - protocolVersion: 2\n                  name: cifar10\n                  type: data\n                  version: latest\n                  contributor: Alice\n                  description: cifar10 dataset, image classification\n                  uri:\n                    - https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n            parameters:\n              type: object\n              description: |\n                If specified, the whole parameters object can be referenced as `$parameters`.\n                Scope of reference `$parameters`: the reference is shared among all task roles.\n                Specify name and value of all the referencable parameters that will be used in the whole job template.\n                Can be referenced by `\u003c% $parameters.param1 %\u003e`, `\u003c% $parameters.param2 %\u003e`.\n              additionalProperties: true\n              example:\n                model: resnet20\n                batchsize: 32\n            secrets:\n              type: object\n              description: |\n                If sensitive information including password or API key is needed in the protocol,\n                it should be specified here in secrets section and referenced as `$secrets`.\n                Scope of reference `$secrets`: the reference is shared among all task roles and docker image's `auth` field.\n                A system that supports PAI protocol should keep the secret information away from\n                unauthorized users (how to define unauthorized user is out of the scope of this protocol).\n                For example, the yaml file used for job cloning, the stdout/stderr should protect all information marked as secrets.\n                Specify name and value of all secrets that will be used in the whole job template.\n                Can be referenced by `\u003c% $secrets.secret1 %\u003e`, `\u003c% $secrets.secret2 %\u003e`.\n              additionalProperties: true\n              example:\n                docker_password: password\n                github_token: cGFzc3dvcmQ=\n            jobRetryCount:\n              type: integer\n              description: Default is 0.\n              minimum: 0\n              example: 1\n            taskRoles:\n              type: object\n              description: |\n                Task roles are different types of task in the protocol. One job may have one or more task roles,\n                each task role has one or more instances, and each instance runs inside one container.\n              additionalProperties:\n                type: object\n                description: Property name of the taskRole should be in ^[a-zA-Z_][a-zA-Z0-9_]*$ format (valid C variable name).\n                properties:\n                  instances:\n                    type: integer\n                    description: Default is 1, instances of a taskRole, no less than 1.\n                    minimum: 1\n                  completion:\n                    type: object\n                    description: Completion poclicy for the job, https://github.com/Microsoft/pai/blob/master/subprojects/frameworklauncher/yarn/doc/USERMANUAL.md#ApplicationCompletionPolicy.\n                    properties:\n                      minFailedInstances:\n                        type: integer\n                        description: |\n                          Number of failed tasks to fail the entire job, -1 or no less than 1,\n                          if set to -1 means the job will always succeed regardless any task failure.\n                          Default is 1.\n                      minSucceededInstances:\n                        type: integer\n                        description: |\n                          Number of succeeded tasks to succeed the entire job, -1 or no less than 1,\n                          if set to -1 means the job will only succeed until all tasks are completed and minFailedInstances is not triggered.\n                          Default is task instances.\n                  taskRetryCount:\n                    type: integer\n                    description: Default is 0.\n                    minimum: 0\n                  prerequisites:\n                    type: array\n                    description: The prerequisites to run this taskrole. These prerequisites will be used in order.\n                    items:\n                      type: string\n                  dockerImage:\n                    type: string\n                    description: Should reference to a dockerimage defined in prerequisites.\n                  data:\n                    type: string\n                    description: |\n                      Select data defined in prerequisites, target can be referenced as `$data` in this task role.\n                      Scope of the reference `$data`, '$output', `$script`: the reference is only valid inside this task role.\n                      User cannot reference them from another task role. Reference for `$parameters` is global and shared among task roles.\n                  output:\n                    type: string\n                    description: Select output defined in prerequisites, target can be referenced as `$output` in this task role.\n                  script:\n                    type: string\n                    description: Select script defined in prerequisites, target can be referenced as `$script` in this task role.\n                  extraContainerOptions:\n                    type: object\n                    properties:\n                      shmMB:\n                        type: integer\n                        description: Config the /dev/shm in a docker container, https://docs.docker.com/compose/compose-file/#shm_size.\n                      infiniband:\n                        type: boolean\n                        description: Use InfiniBand devices or not in a docker container.\n                  resourcePerInstance:\n                    type: object\n                    properties:\n                      cpu:\n                        type: integer\n                        description: CPU number, unit is CPU vcore.\n                      memoryMB:\n                        type: integer\n                        description: Memory number, unit is MB.\n                      gpu:\n                        type: integer\n                        description: GPU number, unit is GPU card.\n                      ports:\n                        type: object\n                        description: Optional, only for host network, property name is string in ^[a-zA-Z_][a-zA-Z0-9_]*$ format (valid C variable name).\n                        additionalProperties:\n                          type: integer\n                          description: Port number for the port label.\n                  commands:\n                    type: array\n                    items:\n                      type: string\n                required:\n                  - dockerImage\n                  - resourcePerInstance\n                  - commands\n              example:\n                worker:\n                  instances: 1\n                  completion:\n                    minFailedInstances: 1\n                    minSucceededInstances: 1\n                  taskRetryCount: 0\n                  dockerImage: tf_example\n                  data: cifar10\n                  output: tensorflow_cifar10_model\n                  script: tensorflow_cnnbenchmarks\n                  extraContainerOptions:\n                    shmMB: 64\n                  resourcePerInstance:\n                    cpu: 2\n                    memoryMB: 16384\n                    gpu: 4\n                    ports:\n                      ssh: 1\n                      http: 1\n                  commands:\n                    - cd script_\u003c% $script.name %\u003e/scripts/tf_cnn_benchmarks\n                    - \u003e\n                      python tf_cnn_benchmarks.py --job_name=worker\n                      --local_parameter_device=gpu\n                      --variable_update=parameter_server\n                      --ps_hosts=$PAI_TASK_ROLE_ps_server_HOST_LIST\n                      --worker_hosts=$PAI_TASK_ROLE_worker_HOST_LIST\n                      --task_index=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX\n                      --data_name=\u003c% $data.name %\u003e\n                      --data_dir=$PAI_WORK_DIR/data_\u003c% $data.name %\u003e\n                      --train_dir=$PAI_WORK_DIR/output_\u003c% $output.name %\u003e\n                      --model=\u003c% $parameters.model %\u003e\n                      --batch_size=\u003c% $parameters.batchsize %\u003e\n                ps_server:\n                  instances: 1\n                  completion:\n                    minFailedInstances: 1\n                    minSucceededInstances: -1\n                  taskRetryCount: 0\n                  dockerImage: tf_example\n                  data: cifar10\n                  output: tensorflow_cifar10_model\n                  script: tensorflow_cnnbenchmarks\n                  extraContainerOptions:\n                    shmMB: 64\n                  resourcePerInstance:\n                    cpu: 2\n                    memoryMB: 8192\n                    gpu: 0\n                    ports:\n                      ssh: 1\n                      http: 1\n                  commands:\n                    - cd script_\u003c% $script.name %\u003e/scripts/tf_cnn_benchmarks\n                    - \u003e\n                      python tf_cnn_benchmarks.py --job_name=ps\n                      --local_parameter_device=gpu\n                      --variable_update=parameter_server\n                      --ps_hosts=$PAI_TASK_ROLE_ps_server_HOST_LIST\n                      --worker_hosts=$PAI_TASK_ROLE_worker_HOST_LIST\n                      --task_index=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX\n                      --data_dir=$PAI_WORK_DIR/data_\u003c% $data.name %\u003e\n                      --data_name=\u003c% $data.name %\u003e\n                      --train_dir=$PAI_WORK_DIR/output_\u003c% $output.name %\u003e\n                      --model=\u003c% $parameters.model %\u003e\n                      --batch_size=\u003c% $parameters.batchsize %\u003e\n            deployments:\n              type: array\n              description: |\n                To handle that a component may interact with different component differently, user is encouraged to place the codes handling such difference in the \"deployments\" field,\n                e.g., a job may get input data through wget, hdfs -dfs cp, copy, or just directly read from remote storage. This logic can be placed here.\n                In summary, the deployments field is responsible to make sure the job to run properly in a deployment specific runtime environment.\n                One could have many deployments, but only one deployment can be activated at runtime by specifying in \"defaults\". User can choose the deployment and specify in \"defaults\" at submission time.\n              items:\n                type: object\n                properties:\n                  name:\n                    type: string\n                  taskRoles:\n                    type: object\n                    additionalProperties:\n                      type: object\n                      description: Property name should be in taskRoles.\n                      properties:\n                        preCommands:\n                          type: array\n                          description: Execute before the taskRole's command.\n                          items:\n                            type: string\n                          minItems: 1\n                        postCommands:\n                          type: array\n                          description: Execute after the taskRole's command.\n                          items:\n                            type: string\n                          minItems: 1\n              required:\n                - name\n              example:\n                # This implementation will download the data to local disk, and the computed model will be output to local disk first and then being copied to hdfs.\n                - name: prod\n                  taskRoles:\n                    worker:\n                      preCommands:\n                        # If local data cache deployed, one can copy data from local cache, only wget in case of cache miss.\n                        - wget \u003c% $data.uri[0] %\u003e -P data_\u003c% $data.name %\u003e\n                        - \u003e\n                          git clone https://\u003c% $script.contributor %\u003e:\u003c% $secrets.github_token %\u003e@\u003c% $script.uri %\u003e script_\u003c% $script.name %\u003e \u0026\u0026\n                          cd script_\u003c% $script.name %\u003e \u0026\u0026 git checkout \u003c% $script.version %\u003e \u0026\u0026 cd ..\n                    ps_server:\n                      preCommands:\n                        - wget \u003c% $data.uri[0] %\u003e -P data_\u003c% $data.name %\u003e\n                        - \u003e\n                          git clone https://\u003c% $script.contributor %\u003e:\u003c% $secrets.github_token %\u003e@\u003c% $script.uri %\u003e script_\u003c% $script.name %\u003e \u0026\u0026\n                          cd script_\u003c% $script.name %\u003e \u0026\u0026 git checkout \u003c% $script.version %\u003e \u0026\u0026 cd ..\n                        # Then the system will go ahead to execute ps_server's command.\n                      postCommands:\n                        # After the execution of ps_server's command, the system goes here.\n                        - hdfs dfs -cp output_\u003c% $output.name %\u003e \u003c% $output.uri %\u003e\n                        # Assume the model is output locally, and this command copies the local output to hdfs. One can output to hdfs directly.\n                        # In this case, you will have to change \"--train_dir=$PAI_WORK_DIR/output_\u003c% $output.name %\u003e\".\n            defaults:\n              type: object\n              description: Optional, default cluster specific settings.\n              properties:\n                virtualCluster:\n                  type: string\n                deployment:\n                  type: string\n                  description: Should reference to deployment defined in deployments.\n              example:\n                # Use prod deployment in job submission.\n                deployment: prod\n            extras:\n              type: object\n              description: Extra field, save any information that plugin may use.\n              properties:\n                submitFrom:\n                  type: string\n                  example: submit-job-v2\n                gangAllocation:\n                  type: boolean\n                  description: Gang scheduling or not.\n                  example: true\n                hivedScheduler:\n                  type: object\n                  description: Hived scheduler configurations.\n                  properties:\n                    jobPriorityClass:\n                      type: string\n                      description: Priority class for the job, oppo is the lowest priority.\n                      enum:\n                        - crit\n                        - prod\n                        - test\n                        - oppo\n                    taskRoles:\n                      type: object\n                      additionalProperties:\n                        type: object\n                        properties:\n                          skuType:\n                            type: string\n                          affinityGroupName:\n                            type: string\n          required:\n            - protocolVersion\n            - name\n            - type\n            - prerequisites\n            - taskRoles\n",
			"canonicalURL": "/github.com/microsoft/openpai-protocol@8cdfc26b336b19ea0720ea7363951dad2a05f72f/-/blob/schemas/v2/schema.yaml",
			"externalURLs": [
				{
					"url": "https://github.com/microsoft/openpai-protocol/blob/8cdfc26b336b19ea0720ea7363951dad2a05f72f/schemas/v2/schema.yaml",
					"serviceKind": "GITHUB"
				}
			]
		}
	}
}