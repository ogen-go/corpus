{
	"File": {
		"__typename": "FileMatch",
		"repository": {
			"name": "github.com/NikolaiT/Crawling-Infrastructure"
		},
		"file": {
			"name": "swagger.json",
			"size": 0,
			"path": "master/src/swagger.json",
			"byteSize": 16468,
			"content": "{\n  \"openapi\": \"3.0.0\",\n  \"info\": {\n    \"version\": \"1.0.1\",\n    \"title\": \"Master Restful Api\",\n    \"description\": \"The Master Rest Api accepts crawl tasks and the scheduler makes progress on them. You can modify existing crawl tasks while they are running and get metadata information about them. Crawl Tasks are defined by you in the form of a Javascript class that derives from `HttpWorker` or `BrowserWorker` and overrides the `crawl` method.\",\n    \"license\": {\n      \"name\": \"MIT\",\n      \"url\": \"https://opensource.org/licenses/MIT\"\n    }\n  },\n  \"tags\": [\n    {\n      \"name\": \"Crawl Tasks\",\n      \"description\": \"API for crawl tasks in the system\"\n    }\n  ],\n  \"paths\": {\n    \"/task\": {\n      \"post\": {\n        \"tags\": [\n          \"Crawl Tasks\"\n        ],\n        \"summary\": \"Create a new crawl task on the  master server\",\n        \"requestBody\": {\n          \"description\": \"Crawl Task Object\",\n          \"required\": true,\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/definitions/CrawlTask\"\n              }\n            }\n          }\n        },\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Crawl Task was successfully added to the infra\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/definitions/CrawlTaskResponse\"\n                }\n              }\n            }\n          },\n          \"400\": {\n            \"description\": \"Failed. Bad post data.\"\n          }\n        }\n      }\n    },\n    \"/task/{id}\": {\n      \"delete\": {\n        \"tags\": [\n          \"Crawl Tasks\"\n        ],\n        \"parameters\": [\n          {\n            \"in\": \"path\",\n            \"name\": \"id\",\n            \"required\": true,\n            \"description\": \"The crawl task id\"\n          }\n        ],\n        \"summary\": \"Delete a task by task id\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Deleted successfully\"\n          },\n          \"400\": {\n            \"description\": \"Failed. Error message.\"\n          }\n        }\n      },\n      \"put\": {\n        \"tags\": [\n          \"Crawl Tasks\"\n        ],\n        \"parameters\": [\n          {\n            \"in\": \"path\",\n            \"name\": \"id\",\n            \"required\": true,\n            \"description\": \"The crawl task id\"\n          }\n        ],\n        \"summary\": \"Update a task by task id\",\n        \"requestBody\": {\n          \"description\": \"Crawl Task Update Object\",\n          \"required\": true,\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/definitions/CrawlTaskUpdate\"\n              }\n            }\n          }\n        },\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Updated successfully\"\n          },\n          \"400\": {\n            \"description\": \"Failed. Error message.\"\n          }\n        }\n      }\n    },\n    \"/stats\": {\n      \"get\": {\n        \"tags\": [\n          \"Crawl Tasks\"\n        ],\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"API_KEY\",\n            \"required\": true,\n            \"description\": \"The API KEY to auth the request\"\n          },\n          {\n            \"in\": \"query\",\n            \"name\": \"simple\",\n            \"required\": false,\n            \"description\": \"Whether to print a simple anf fast summary or a extensive and slow summary. Use `simple=false` for an extensive summary.\"\n          }\n        ],\n        \"summary\": \"Get statistical information about all crawl tasks in the system\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/definitions/CrawlTaskStats\"\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"/proxies\": {\n      \"get\": {\n        \"tags\": [\n          \"Crawl Tasks\"\n        ],\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"API_KEY\",\n            \"required\": true,\n            \"description\": \"The API KEY to auth the request\"\n          },\n          {\n            \"in\": \"query\",\n            \"name\": \"filter\",\n            \"required\": false,\n            \"description\": \"Filter the proxy by adding query args.\",\n            \"example\": \"?status=0\u0026provider=cosmoproxy\"\n          }\n        ],\n        \"summary\": \"Get all proxies currently loaded into the crawling infra.\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/definitions/Proxy\"\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"/storage/{id}\": {\n      \"get\": {\n        \"tags\": [\n          \"Crawl Tasks\"\n        ],\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"API_KEY\",\n            \"required\": true,\n            \"description\": \"The API KEY to auth the request\"\n          },\n          {\n            \"in\": \"query\",\n            \"name\": \"how\",\n            \"required\": false,\n            \"description\": \"how the s3 storage results should be displayed\",\n            \"example\": \"use `?how=cmd` in order to obtain download instructions as bash script and `?how=flat` in order to get s3 locations\"\n          },\n          {\n            \"in\": \"path\",\n            \"name\": \"id\",\n            \"required\": true,\n            \"description\": \"The crawl task id\"\n          }\n        ],\n        \"summary\": \"Get storage information for crawl tasks.\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/definitions/Proxy\"\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"/results/{id}\": {\n      \"get\": {\n        \"tags\": [\n          \"Crawl Tasks\"\n        ],\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"API_KEY\",\n            \"required\": true,\n            \"description\": \"The API KEY to auth the request\"\n          },\n          {\n            \"in\": \"path\",\n            \"name\": \"id\",\n            \"required\": true,\n            \"description\": \"The crawl task id\"\n          }\n        ],\n        \"summary\": \"Obtain crawl task results.\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Contains the results payload\"\n          },\n          \"400\": {\n            \"description\": \"Failed. Error message.\"\n          }\n        }\n      }\n    }\n  },\n  \"definitions\": {\n    \"CrawlTask\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"items\": {\n          \"type\": \"string\",\n          \"required\": true,\n          \"description\": \"A url or local path to a separated text file holding one item per line\"\n        },\n        \"function\": {\n          \"type\": \"string\",\n          \"required\": true,\n          \"description\": \"A url or local path to a text file with a javascript class that either extends HttpWorker or BrowserWorker\"\n        },\n        \"worker_type\": {\n          \"type\": \"string\",\n          \"enum\": [\"http\", \"browser\"],\n          \"required\": true,\n          \"description\": \"What kind of worker your crawl task is\"\n        },\n        \"regions\": {\n          \"type\": \"string\",\n          \"default\": \"\",\n          \"required\": false,\n          \"description\": \"Add a region from where to run your crawl task. If not specified, regions are chosen arbitrarily\"\n        },\n        \"priority\": {\n          \"type\": \"number\",\n          \"required\": false,\n          \"default\": 1,\n          \"description\": \"The priority of this crawl task. 1 is lowest priority, 10 is highest priority\"\n        },\n        \"priority_policy\": {\n          \"type\": \"string\",\n          \"enum\": [\"absolute\", \"relative\"],\n          \"required\": false,\n          \"default\": \"absolute\",\n          \"description\": \"How priority is handled. When set to `absolute`, crawl tasks with priority 1 will never receive crawling resources until a task with priority 2 or higher is finished. When using `relative`, tasks will receive crawling resources relative to their priority\"\n        },\n        \"max_items_per_second\": {\n          \"type\": \"number\",\n          \"default\": 1,\n          \"required\": false,\n          \"description\": \"The maximal throughput in terms of items/second that this crawl task should achieve.\"\n        },\n        \"crawl_options\": {\n          \"$ref\": \"#/definitions/CrawlOptions\"\n        },\n        \"allow_duplicates\": {\n          \"type\": \"boolean\",\n          \"default\": true,\n          \"required\": false,\n          \"description\": \"Whether duplicate items should be removed or not\"\n        },\n        \"whitelisted_proxies\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"required\": false,\n          \"description\": \"Whether the task must have access to whitelisted proxies. Worker will be spawned as part of a docker cluster as a consequence.\"\n        }\n      }\n    },\n    \"Test\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"simple\": {\n          \"type\": \"string\",\n          \"description\": \"A simple string.\"\n        }\n      }\n    },\n    \"CrawlOptions\": {\n      \"type\": \"object\",\n      \"required\": [],\n      \"properties\": {\n        \"user_agent\": {\n          \"type\": \"string\",\n          \"description\": \"The default user agent to be used with this crawl task. All requests will use this user agent.\",\n          \"default\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3942.0 Safari/537.36\"\n        },\n        \"headers\": {\n          \"type\": \"array\",\n          \"default\": [],\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"description\": \"The default headers to be used with this crawl task. All requests will use those headers.\"\n        },\n        \"random_user_agent\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"Whether a random user agent should be used for each new allocation of a puppeteer browser or Got client library.\"\n        },\n        \"request_timeout\": {\n          \"type\": \"number\",\n          \"default\": 15000,\n          \"description\": \"The timeout in milliseconds before a Got request is deemed to have failed.\"\n        },\n        \"cookies\": {\n          \"type\": \"array\",\n          \"default\": [],\n          \"items\": {\n            \"$ref\": \"#/definitions/Cookie\"\n          },\n          \"description\": \"Those cookies are added to the http or browser context of each request\"\n        },\n        \"viewport\": {\n          \"type\": \"string\",\n          \"default\": \"\",\n          \"description\": \"The viewport of the chromium frame when crawling\"\n        },\n        \"default_navigation_timeout\": {\n          \"type\": \"number\",\n          \"default\": 40000,\n          \"description\": \"The navigation timeout for browser crawling\"\n        },\n        \"pup_args\": {\n          \"type\": \"array\",\n          \"default\": [],\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"description\": \"An array of additional puppeteer command line arguments that are passed to the chromium browser. See here: https://peter.sh/experiments/chromium-command-line-switches/\"\n        },\n        \"intercept_types\": {\n          \"type\": \"array\",\n          \"example\": [\"\"],\n          \"default\": [],\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"description\": \"What request media types should be aborted. Helps to save bandwith and time. Allowed values: ['image','stylesheet','media','font','texttrack','object','beacon','csp_report','imageset']\"\n        },\n        \"apply_evasion\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"Whether well known puppeteer detection evasion techniques should be used. This library is used internally: https://www.npmjs.com/package/puppeteer-extra-plugin-stealth\"\n        },\n        \"userDataDir\": {\n          \"type\": \"string\",\n          \"example\": \"/tmp/tmpDataDir/\"\n        },\n        \"block_webrtc\": {\n          \"type\": \"boolean\"\n        }\n      }\n    },\n    \"Cookie\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"Cookie name\"\n        },\n        \"value\": {\n          \"type\": \"string\",\n          \"description\": \"Cookie value\"\n        },\n        \"domain\": {\n          \"type\": \"string\",\n          \"description\": \"Cookie domain\"\n        }\n      }\n    },\n    \"CrawlTaskResponse\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"id\": {\n          \"type\": \"string\"\n        },\n        \"num_items\": {\n          \"type\": \"number\"\n        },\n        \"created_at\": {\n          \"type\": \"string\"\n        },\n        \"status\": {\n          \"type\": \"number\",\n          \"enum\": [\"started\", \"completed\", \"failed\", \"paused\"]\n        },\n        \"regions\": {\n          \"type\": \"number\"\n        }\n      }\n    },\n    \"CrawlTaskRps\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"id\": {\n          \"type\": \"string\",\n          \"required\": true\n        },\n        \"max_items_per_second\": {\n          \"type\": \"number\",\n          \"required\": true,\n          \"description\": \"The throughput in terms of number of items/second that should be achieved by the crawl task.\"\n        }\n      }\n    },\n    \"CrawlTaskStats\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"id\": {\n          \"type\": \"string\"\n        },\n        \"status\": {\n          \"type\": \"number\",\n          \"enum\": [\"started\", \"completed\", \"failed\", \"paused\"]\n        },\n        \"num_items\": {\n          \"type\": \"number\"\n        },\n        \"max_items_per_second\": {\n          \"type\": \"number\"\n        },\n        \"average_rps\": {\n          \"type\": \"string\"\n        },\n        \"num_crawl_workers_started\": {\n          \"type\": \"number\"\n        },\n        \"num_workers_running\": {\n          \"type\": \"number\"\n        },\n        \"num_lost_workers\": {\n          \"type\": \"number\"\n        },\n        \"worker_meta_statistics\": {\n          \"type\": \"object\"\n        },\n        \"queue_statistics\": {\n          \"type\": \"object\"\n        }\n      }\n    },\n    \"CrawlTaskUpdate\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"status\": {\n          \"type\": \"number\",\n          \"enum\": [\"started\", \"completed\", \"failed\", \"paused\"]\n        },\n        \"max_items_per_second\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 50\n        },\n        \"max_lost_workers\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 10000\n        },\n        \"max_items_per_worker\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 1000\n        },\n        \"priority\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 10\n        },\n        \"priority_policy\": {\n          \"type\": \"number\",\n          \"enum\": [\"absolute\", \"relative\"]\n        }\n      }\n    },\n    \"Proxy\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\"datacenter\", \"dedicated\", \"residential\", \"mobile\"]\n        },\n        \"provider\": {\n          \"type\": \"string\",\n          \"enum\": [\"luminati\", \"cosmoproxy\", \"stormproxies\"]\n        },\n        \"status\": {\n          \"type\": \"number\",\n          \"enum\": [\"functional\", \"damaged\", \"expired\"]\n        },\n        \"subtype\": {\n          \"type\": \"string\",\n          \"example\": \"linkedin\"\n        },\n        \"proxy\": {\n          \"type\": \"string\",\n          \"example\": \"127.0.0.0:9005\"\n        },\n        \"protocol\": {\n          \"type\": \"string\",\n          \"enum\": [\"http\", \"https\", \"socks4\", \"socks5\" ,\"other\"]\n        },\n        \"whitelisted\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"Proxy is whitelisted.\"\n        },\n        \"rotating\": {\n          \"type\": \"boolean\",\n          \"description\": \"Proxy is rotating.\"\n        },\n        \"username\": {\n          \"type\": \"string\"\n        },\n        \"password\": {\n          \"type\": \"string\"\n        },\n        \"last_used\": {\n          \"type\": \"string\",\n          \"description\": \"When the proxy was last used, successful or unsuccessful.\"\n        },\n        \"last_blocked\": {\n          \"type\": \"string\",\n          \"description\": \"When the proxy was last blocked.\"\n        },\n        \"block_counter\": {\n          \"type\": \"string\",\n          \"description\": \"How many times was the proxy blocked\"\n        }\n      }\n    }\n  }\n}"
		}
	},
	"Error": "parse: parse spec: parse operations: paths: /results/{id}: get: parameters: parse parameter \"API_KEY\": parameter MUST contain either a schema property, or a content property"
}