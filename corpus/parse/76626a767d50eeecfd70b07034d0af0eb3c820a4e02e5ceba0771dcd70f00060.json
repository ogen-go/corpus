{
	"File": {
		"__typename": "FileMatch",
		"repository": {
			"name": "github.com/deepviss/deepviss-server-example"
		},
		"file": {
			"name": "deep-viss-openapi-v3.json",
			"size": 0,
			"path": "src/main/resources/deep-viss-openapi-v3.json",
			"byteSize": 31678,
			"content": "{\n  \"openapi\": \"3.0.0\",\n  \"info\": {\n    \"description\": \"DeepVISS (Deep Vision Interoperability Specification Standard) allows several computer vision solutions to produce, consume and exchange events in the same format.\",\n    \"version\": \"1.3.0\",\n    \"title\": \"DeepVISS OPS\",\n    \"termsOfService\": \"https://deepviss.org\",\n    \"contact\": {\n      \"email\": \"office@deepviss.org\"\n    },\n    \"license\": {\n      \"name\": \"Apache 2.0\",\n      \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\"\n    }\n  },\n  \"tags\": [\n    {\n      \"name\": \"frame\",\n      \"description\": \"Information about post-processed frames\",\n      \"externalDocs\": {\n        \"description\": \"Streaming API\",\n        \"url\": \"https://deepviss.org\"\n      }\n    },\n    {\n      \"name\": \"analysis\",\n      \"description\": \"End-points for image or image array analysis\",\n      \"externalDocs\": {\n        \"description\": \"Find out more\",\n        \"url\": \"https://deepviss.org\"\n      }\n    }\n  ],\n  \"paths\": {\n    \"/stream/Frames\": {\n      \"get\": {\n        \"tags\": [\n          \"frame\"\n        ],\n        \"summary\": \"Retrieve last frames\",\n        \"description\": \"Multiple status values can be provided with comma separated strings\",\n        \"operationId\": \"GetLastFrames\",\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"streamId\",\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The identifier of the stream\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"successful operation\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"array\",\n                  \"items\": {\n                    \"$ref\": \"#/definitions/Frame\"\n                  }\n                }\n              }\n            }\n          },\n          \"400\": {\n            \"description\": \"Invalid status value\"\n          }\n        }\n      }\n    },\n    \"/analysis/ProcessingRequest\": {\n      \"post\": {\n        \"tags\": [\n          \"analysis\"\n        ],\n        \"summary\": \"Analyze a specific frame\",\n        \"description\": \"\",\n        \"operationId\": \"ProcessingRequest\",\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"streamId\",\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The identifier of the stream\"\n          }\n        ],\n        \"requestBody\": {\n          \"description\": \"Deliver the image to be analyzed *Markdown*\",\n          \"required\": true,\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/ProcessingRequest\"\n              }\n            },\n            \"application/xml\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/ProcessingRequest\"\n              }\n            }\n          }\n        },\n        \"responses\": {\n          \"200\": {\n            \"description\": \"successful operation\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"array\",\n                  \"items\": {\n                    \"$ref\": \"#/definitions/Frame\"\n                  }\n                }\n              }\n            }\n          },\n          \"400\": {\n            \"description\": \"Invalid status value\"\n          }\n        }\n      }\n    },\n    \"/analysis/ImagesArray\": {\n      \"post\": {\n        \"tags\": [\n          \"analysis\"\n        ],\n        \"summary\": \"Analyze an array of images\",\n        \"description\": \"\",\n        \"operationId\": \"ImageArrayAnalysis\",\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"streamId\",\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The identifier of the stream\"\n          }\n        ],\n        \"requestBody\": {\n          \"description\": \"Deliver the image to be analyzed *Markdown*\",\n          \"required\": true,\n          \"content\": {\n            \"multipart/form-data\": {\n              \"schema\": {\n                \"properties\": {\n                  \"filename\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\",\n                      \"format\": \"binary\"\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"responses\": {\n          \"200\": {\n            \"description\": \"successful operation\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"array\",\n                  \"items\": {\n                    \"$ref\": \"#/definitions/Frame\"\n                  }\n                }\n              }\n            }\n          },\n          \"400\": {\n            \"description\": \"Invalid status value\"\n          }\n        }\n      }\n    },\n    \"/analysis/Image\": {\n      \"post\": {\n        \"tags\": [\n          \"analysis\"\n        ],\n        \"summary\": \"Analyze a single image\",\n        \"description\": \"\",\n        \"operationId\": \"ImageAnalysis\",\n        \"parameters\": [\n          {\n            \"in\": \"query\",\n            \"name\": \"streamId\",\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The identifier of the stream\"\n          }\n        ],\n        \"requestBody\": {\n          \"description\": \"Deliver the image to be analyzed *Markdown*\",\n          \"required\": true,\n          \"content\": {\n            \"multipart/form-data\": {\n              \"schema\": {\n                \"properties\": {\n                  \"filename\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\",\n                      \"format\": \"binary\"\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"responses\": {\n          \"200\": {\n            \"description\": \"successful operation\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"array\",\n                  \"items\": {\n                    \"$ref\": \"#/definitions/Frame\"\n                  }\n                }\n              }\n            }\n          },\n          \"400\": {\n            \"description\": \"Invalid status value\"\n          }\n        }\n      }\n    }\n  },\n  \"components\": {\n    \"schemas\": {\n      \"Frame\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"timestamps\": {\n            \"description\": \"Defines the several timestamps of acquisition, reception, pre-processing and post-processing respectively.\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"FrameTimestamp\"\n            }\n          },\n          \"frameId\": {\n            \"type\": \"string\",\n            \"description\": \"Alpha-numeric, unique id of frame. You can use (timestamp+sourceId) or sha512(timestamp+sourceId)\",\n            \"example\": \"7E43358680768EF053898993DD196397EDFDDFBC81751818B7FD1300124455B07E91CB289F87791D78064ECC93754F19B13D419489F162A150A22DD814CKAF0E\"\n          },\n          \"frameCounter\": {\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"description\": \"Per-source incremental frame counter\",\n            \"example\": 86537\n          },\n          \"sourceId\": {\n            \"type\": \"string\",\n            \"description\": \"Video source id.\"\n          },\n          \"image\": {\n            \"type\": \"Image\",\n            \"description\": \"Image information\"\n          },\n          \"events\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"Event\"\n            }\n          },\n          \"timing\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\n              \"type\": \"double\"\n            }\n          },\n          \"debug\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\n              \"type\": \"string\"\n            }\n          }\n        }\n      },\n      \"Image\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"imageURL\": {\n            \"description\": \"The URL where the image is stored.\",\n            \"type\": \"string\"\n          },\n          \"imageBase64\": {\n            \"type\": \"string\",\n            \"format\": \"byte\",\n            \"description\": \"Base64 encoded string of the image.\"\n          },\n          \"imageContentType\": {\n            \"type\": \"string\",\n            \"description\": \"Image MIME-type, such as image/png or image/jpeg\"\n          }\n        }\n      },\n      \"FrameTimestamp\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"value\": {\n            \"type\": \"string\",\n            \"pattern\": \"/^(-?(?:[1-9][0-9]*)?[0-9]{4})-(1[0-2]|0[1-9])-(3[01]|0[1-9]|[12][0-9])T(2[0-3]|[01][0-9]):([0-5][0-9]):([0-5][0-9])(.[0-9]+)?(Z)?$/g\",\n            \"description\": \"Date and time expressed according to ISO 8601 (e.g. 2018-06-24T23:10:28+03:00)\"\n          },\n          \"reference\": {\n            \"type\": \"string\",\n            \"enum\": [\n              \"acquisition\",\n              \"reception\",\n              \"preprocessing\",\n              \"postprocessing\"\n            ]\n          }\n        }\n      },\n      \"Event\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"objectType\": {\n            \"type\": \"string\",\n            \"description\": \"What type of event has been detected?\",\n            \"example\": \"vehicle\"\n          },\n          \"id\": {\n            \"type\": \"string\",\n            \"description\": \"Unique or almost-unique hash or identifier of the detected event. Can be computed as dependant on frame timestamp, position of detection and event type.\",\n            \"example\": \"T2NLg96Xosg7NLg96Xos\"\n          },\n          \"attributes\": {\n            \"description\": \"What are the inferred attributes of the object?\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"Attribute\"\n            }\n          },\n          \"tracking\": {\n            \"type\": \"Tracking\",\n            \"description\": \"Tracking id and tracking detection\"\n          },\n          \"detection\": {\n            \"type\": \"Detection\",\n            \"description\": \"Detection information\"\n          },\n          \"features\": {\n            \"type\": \"Features\",\n            \"description\": \"[DEPRECATED] The main features of detectected object.\"\n          },\n          \"featuresMap\": {\n            \"description\": \"What are the secondary features extracted from the detected object?\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"Features\"\n            }\n          },\n          \"processedImage\": {\n            \"type\": \"Image\",\n            \"description\": \"Any post-processing image returned for this event.\"\n          }\n        }\n      },\n      \"Detection\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"boundingRectangle\": {\n            \"type\": \"BoundingRectangle\",\n            \"description\": \"Where in the frame has the object been detected?\"\n          },\n          \"orientation\": {\n            \"type\": \"Orientation\",\n            \"description\": \"What is the geometric orientation of the detected object or event?\"\n          },\n          \"algorithm\": {\n            \"type\": \"string\",\n            \"description\": \"What type of algorithm performed the detection\",\n            \"example\": \"faster-rcnn\"\n          },\n          \"keypoints\": {\n            \"description\": \"What are the keypoints of the object??\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"Point2D\"\n            }\n          },\n          \"segmentedKeypoints\": {\n            \"description\": \"What are the keypoints of the object??\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"KeypointsSegment\"\n            }\n          },\n          \"confidence\": {\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"description\": \"What is the confidence of the detection\",\n            \"example\": 0.9731\n          }\n        }\n      },\n      \"BoundingRectangle\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"top\",\n          \"left\",\n          \"width\",\n          \"height\"\n        ],\n        \"properties\": {\n          \"top\": {\n            \"description\": \"Top-most position, in pixels, of of the bounding rectangle\",\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"example\": 32\n          },\n          \"left\": {\n            \"description\": \"Left-most position, in pixels, of of the bounding rectangle\",\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"example\": 57\n          },\n          \"width\": {\n            \"description\": \"Width, in pixels, of the bounding rectangle\",\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"example\": 237\n          },\n          \"height\": {\n            \"description\": \"Height, in pixels, of the bounding rectangle\",\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"example\": 352\n          }\n        }\n      },\n      \"Point2D\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"x\",\n          \"y\"\n        ],\n        \"properties\": {\n          \"x\": {\n            \"description\": \"X-coordinate as integer number of pixels measured from the left to the right.\",\n            \"type\": \"integer\",\n            \"format\": \"int32\"\n          },\n          \"y\": {\n            \"description\": \"Y-coordinate as integer number of pixels measured from the top to the bottom.\",\n            \"type\": \"integer\",\n            \"format\": \"int32\"\n          }\n        }\n      },\n      \"SourceStatus\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"id\",\n          \"name\"\n        ],\n        \"properties\": {\n          \"ingestedFrame\": {\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"description\": \"Number of frame which have been successfully captured.\",\n            \"example\": 86537\n          },\n          \"processedFrames\": {\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"description\": \"Number of frame which have been successfully processed.\",\n            \"example\": 86537\n          },\n          \"ingestedErrorCount\": {\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"description\": \"Number of errors, dropped or missed frames from acquisition\",\n            \"example\": 86537\n          },\n          \"processedErrorCount\": {\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"description\": \"Number of errors, dropped or missed frames from processing\",\n            \"example\": 86537\n          }\n        }\n      },\n      \"Source\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"id\",\n          \"name\"\n        ],\n        \"properties\": {\n          \"id\": {\n            \"description\": \"Internal alphanumeric ID of video source\",\n            \"type\": \"string\"\n          },\n          \"name\": {\n            \"description\": \"Human-readable name of video source\",\n            \"type\": \"string\"\n          },\n          \"vendor\": {\n            \"description\": \"Vendor of the camera\",\n            \"type\": \"string\"\n          },\n          \"model\": {\n            \"description\": \"Model of the camera\",\n            \"type\": \"string\"\n          },\n          \"stateless\": {\n            \"description\": \"Set to true if stream is expected to have non-continous frames.\",\n            \"type\": \"boolean\"\n          },\n          \"bounded\": {\n            \"description\": \"Set to true if stream represents a finite-duration video (like a movie upload)\",\n            \"type\": \"boolean\"\n          },\n          \"fixedPosition\": {\n            \"description\": \"Set to true if camera is expected to move.\",\n            \"type\": \"boolean\"\n          },\n          \"Position\": {\n            \"description\": \"The geographic (physical) position of the camera\",\n            \"type\": \"GeoPosition\"\n          },\n          \"connection\": {\n            \"description\": \"Details on connection.\",\n            \"type\": \"SourceConnection\"\n          }\n        }\n      },\n      \"Orientation\": {\n        \"type\": \"object\",\n        \"required\": [\n        ],\n        \"properties\": {\n          \"yaw\": {\n            \"description\": \"The yaw of the detected object or event.\",\n            \"type\": \"number\",\n            \"format\": \"double\"\n          },\n          \"pitch\": {\n            \"description\": \"The pitch of the detected object or event.\",\n            \"type\": \"number\",\n            \"format\": \"double\"\n          },\n          \"roll\": {\n            \"description\": \"The roll of the detected object or event.\",\n            \"type\": \"number\",\n            \"format\": \"double\"\n          }\n        }\n      },\n      \"Metric\": {\n        \"description\": \"The recommended type of metric used \",\n        \"type\": \"string\",\n        \"example\": \"Euclidean tripartite loss function\",\n        \"enum\": [\n          \"euclidean\",\n          \"manhattan\",\n          \"cosine\"\n        ]\n      },\n      \"Features\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"features\"\n        ],\n        \"properties\": {\n          \"algorithm\": {\n            \"description\": \"The name and version of the algorithm used for feature extraction\",\n            \"type\": \"string\",\n            \"example\": \"Euclidean tripartite loss function\"\n          },\n          \"privacy\": {\n            \"description\": \"The recommended type of metric used \",\n            \"type\": \"PrivacyIndication\"\n          },\n          \"metric\": {\n            \"description\": \"The recommended type of metric used \",\n            \"type\": \"Metric\"\n          },\n          \"threshold\": {\n            \"description\": \"The value of the metric threshold\",\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"example\": 0.8\n          },\n          \"hash\": {\n            \"description\": \"The hash of the features, for quick exact-match look-up\",\n            \"type\": \"string\",\n            \"example\": \"8e347f952424ec9dadb61ab399074362ade84f76f2e5e78de41978d1baa8aaadaace00ec5f93e57c8bfb92d07d5c248873f9ac7964027c80a4e4d0e88fa56459\"\n          },\n          \"vector\": {\n            \"description\": \"N-dimensional feature vector of numerical features representing the unique signature of the object\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"number\",\n              \"format\": \"double\"\n            }\n          }\n        }\n      },\n      \"Attribute\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"name\",\n          \"value\"\n        ],\n        \"properties\": {\n          \"name\": {\n            \"description\": \"The name of the attribute\",\n            \"type\": \"string\",\n            \"example\": \"age-group\"\n          },\n          \"value\": {\n            \"description\": \"The value of the attribute\",\n            \"type\": \"string\",\n            \"example\": \"32-45\"\n          },\n          \"confidence\": {\n            \"description\": \"The confidence of the classification\",\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"example\": 0.93\n          },\n          \"type\": {\n            \"description\": \"The type of the attribute, to be used in higher order processing and logic.\",\n            \"type\": \"string\",\n            \"enum\": [\n              \"lexicographic\",\n              \"hex-color\",\n              \"integer\",\n              \"double\"\n            ]\n          }\n        }\n      },\n      \"ProcessingRequest\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"image\": {\n            \"type\": \"Image\",\n            \"description\": \"Image information\"\n          },\n          \"regionsOfInterest\": {\n            \"description\": \"Define any regions of interest, defined by bounding boxes, where the processing should focus.\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"BoundingRectangle\"\n            }\n          },\n          \"parameters\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\n              \"type\": \"string\"\n            }\n          },\n          \"frameId\": {\n            \"type\": \"string\",\n            \"description\": \"Alpha-numeric, unique id of frame. You can use (timestamp+sourceId) or sha512(timestamp+sourceId)\",\n            \"example\": \"7E43358680768EF053898993DD196397EDFDDFBC81751818B7FD1300124455B07E91CB289F87791D78064ECC93754F19B13D419489F162A150A22DD814CKAF0E\"\n          },\n          \"frameCounter\": {\n            \"type\": \"integer\",\n            \"format\": \"int32\",\n            \"description\": \"Per-source incremental frame counter\",\n            \"example\": 86537\n          },\n          \"sourceId\": {\n            \"type\": \"string\",\n            \"description\": \"Video source id.\"\n          }\n        }\n      },\n      \"GeoPosition\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"latitude\",\n          \"longitude\"\n        ],\n        \"properties\": {\n          \"latitude\": {\n            \"description\": \"The latitude at which the video source is installed.\",\n            \"type\": \"number\",\n            \"format\": \"double\"\n          },\n          \"longitude\": {\n            \"description\": \"The longitude at which the video source is installed.\",\n            \"type\": \"number\",\n            \"format\": \"double\"\n          },\n          \"altitude\": {\n            \"description\": \"The altitude at which the video source is installed.\",\n            \"type\": \"number\",\n            \"format\": \"double\"\n          },\n          \"elevation\": {\n            \"description\": \"The elevation from the ground at which the video source is installed, measured in meters.\",\n            \"type\": \"number\",\n            \"format\": \"double\"\n          }\n        }\n      },\n      \"KeypointsSegment\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"keypoints\" : {\n            \"description\" : \"Collection of key-points, grouped in a map with keys representing names and values represented by a value of points.\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"Point2D\"\n            }\n          }\n        }\n      },\n      \"SourceConnection\": {\n        \"type\": \"object\",\n        \"required\": [\n          \"url\",\n          \"protocol\"\n        ],\n        \"properties\": {\n          \"url\": {\n            \"description\": \"The hostname of the video source\",\n            \"type\": \"string\"\n          },\n          \"username\": {\n            \"description\": \"The username used for authentication to the video source\",\n            \"type\": \"string\"\n          },\n          \"password\": {\n            \"description\": \"The password used for authentication to the video source\",\n            \"type\": \"string\"\n          },\n          \"protocol\": {\n            \"type\": \"string\",\n            \"description\": \"The protocol used to connect to the video source\",\n            \"enum\": [\n              \"jpeg_snapshot\",\n              \"mjpeg\",\n              \"h264_http\",\n              \"hls\",\n              \"webrtc\",\n              \"image_push\",\n              \"video_push\"\n            ]\n          }\n        }\n      },\n      \"ObjectCollection\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"description\": \"The ID of the collection\",\n            \"type\": \"string\"\n          },\n          \"name\": {\n            \"description\": \"The human-readable name of the collection\",\n            \"type\": \"string\"\n          },\n          \"objectType\": {\n            \"description\": \"The type of objects in the collection. A collection can only hold objects of the same type.\",\n            \"type\": \"string\"\n          },\n          \"algorithm\": {\n            \"description\": \"The name and version of the algorithm used for feature extraction. A collection can only hold objects with the same feature space (latent space).\",\n            \"type\": \"string\",\n            \"example\": \"Euclidean tripartite loss function\"\n          },\n          \"metric\": {\n            \"description\": \"The metric used for comparing collection objects \",\n            \"type\": \"Metric\"\n          },\n          \"threshold\": {\n            \"description\": \"The recommended metric value recommended to be used as threshold\",\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"example\": 0.8\n          },\n          \"profiles\": {\n            \"type\": \"ObjectProfile\",\n            \"description\": \"List of ObjectProfiles. All profiles must be of the same type as the collection object type.\"\n          }\n        }\n      },\n      \"ObjectProfile\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"description\": \"The ID of the object profile, containing several ObjectInstance. ObjectProfiles belong to ObjectCollections.\",\n            \"type\": \"string\"\n          },\n          \"externalId\": {\n            \"description\": \"The ID of the object profile, containing several ObjectInstance. ObjectProfiles belong to ObjectCollections.\",\n            \"type\": \"string\"\n          },\n          \"collectionId\": {\n            \"description\": \"The ID of parent collection.\",\n            \"type\": \"string\"\n          },\n          \"avatars\": {\n            \"description\": \"The list of object instances statically assigned to the object profile.\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"string\"\n            }\n          },\n          \"displayName\": {\n            \"description\": \"The human-readable name of the profile\",\n            \"type\": \"string\"\n          },\n          \"instances\": {\n            \"description\": \"The list of object instances statically assigned to the object profile.\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"ObjectInstance\"\n            }\n          },\n          \"attributes\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\n              \"type\": \"string\"\n            }\n          },\n          \"occurrences\": {\n            \"description\": \"The list of object instances dynamically assigned to the object profile.\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"ObjectOccurrence\"\n            }\n          }\n        }\n      },\n      \"ObjectInstance\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"description\": \"The ID of the object instance, containing the feature vector. ObjectInstances belong to ObjectProfiles, which in turn belong to ObjectCollections.\",\n            \"type\": \"string\"\n          },\n          \"profileId\": {\n            \"description\": \"The ID of parent profile.\",\n            \"type\": \"string\"\n          },\n          \"image\": {\n            \"type\": \"Image\",\n            \"description\": \"Original image information associated with the ObjectInstance\"\n          },\n          \"features\": {\n            \"type\": \"Features\",\n            \"description\": \"What are the features extracted from the detected object?\"\n          }\n        }\n      },\n      \"ObjectOccurrence\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"sourceId\": {\n            \"description\": \"The ID of the source from which the frame originated\",\n            \"type\": \"string\"\n          },\n          \"timestamp\": {\n            \"type\": \"FrameTimestamp\",\n            \"description\": \"The timestamp of the frame from which match originated.\"\n          },\n          \"eventId\": {\n            \"description\": \"The ID of the object instance\",\n            \"type\": \"string\"\n          },\n          \"objectInstanceId\": {\n            \"type\": \"string\",\n            \"description\": \"The id of the object instance.\"\n          },\n          \"matches\": {\n            \"description\": \"The list of profile that object matched as compared to a list of collections, at a specific time\",\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"ObjectMatch\"\n            }\n          }\n        }\n      },\n      \"ObjectMatch\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"matchType\": {\n            \"description\": \"The match with an object\",\n            \"type\": \"string\"\n          },\n          \"algorithm\": {\n            \"description\": \"The name and version of the algorithm used for feature extraction\",\n            \"type\": \"string\",\n            \"example\": \"Euclidean tripartite loss function\"\n          },\n          \"metric\": {\n            \"description\": \"The ID of the collection from which the match was made\",\n            \"type\": \"string\"\n          },\n          \"collectionId\": {\n            \"description\": \"The ID of the collection from which the match was made\",\n            \"type\": \"string\"\n          },\n          \"profileId\": {\n            \"description\": \"The ID of the profile that was matched.\",\n            \"type\": \"string\"\n          },\n          \"matchRate\": {\n            \"description\": \"The ID of the profile that was matched.\",\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"example\": 0.79\n          }\n        }\n      },\n      \"ObjectBucket\": {\n        \"description\": \"[TODO: Object definition] Unsorted and unassigned set of object occurences, which can server for clustering and later historical-matching.\",\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"description\": \"The ID of the bucket of object instances\",\n            \"type\": \"string\"\n          }\n        }\n      },\n      \"Transcription\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"text\": {\n            \"description\": \"The ID of the collection from which the match was made\",\n            \"type\": \"string\"\n          },\n          \"confidence\": {\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"example\": 0.83\n          }\n        }\n      },\n      \"Tracking\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"trackingId\": {\n            \"description\": \"The tracking Id of the object\",\n            \"type\": \"string\"\n          },\n          \"contextId\": {\n            \"description\": \"The context from which the tracking originated\",\n            \"type\": \"string\"\n          },\n          \"confidence\": {\n            \"description\": \"The confidence that the object is tracked correctly and consistently.\",\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"example\": 0.83\n          }\n        }\n      },\n      \"PrivacyIndication\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"hasBiometricData\": {\n            \"description\": \"Does object contain biometric data?\",\n            \"type\": \"boolean\"\n          },\n          \"retentionAllowed\": {\n            \"description\": \"Is retention of the object allowed?\",\n            \"type\": \"boolean\"\n          },\n          \"scheduledEvictionTime\": {\n            \"type\": \"string\",\n            \"pattern\": \"/^(-?(?:[1-9][0-9]*)?[0-9]{4})-(1[0-2]|0[1-9])-(3[01]|0[1-9]|[12][0-9])T(2[0-3]|[01][0-9]):([0-5][0-9]):([0-5][0-9])(.[0-9]+)?(Z)?$/g\",\n            \"description\": \"Date and time expressed according to ISO 8601 (e.g. 2018-06-24T23:10:28+03:00) of when this data is expected to be collected.\"\n          }\n        }\n      },\n      \"PackedPrefetch\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"profiles\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"ObjectProfile\"\n            }\n          }\n        }\n      },\n      \"PackedSight\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"frames\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"Frame\"\n            }\n          },\n          \"occurrences\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"ObjectOccurrence\"\n            }\n          },\n          \"prefetch\": {\n            \"type\": \"PackedPrefetch\"\n          }\n        }\n      }\n    }\n  },\n  \"externalDocs\": {\n    \"description\": \"Deep vision interoperability specification standard (DeepVISS) reduces integration time, complexity and defects by using a standardized, language-agnostic, cross-platform schema\",\n    \"url\": \"https://deepviss.org\"\n  }\n}",
			"canonicalURL": "/github.com/deepviss/deepviss-server-example@3ea50ce849c628c9cffa8657e4db9a663fd6b3d8/-/blob/src/main/resources/deep-viss-openapi-v3.json",
			"externalURLs": [
				{
					"url": "https://github.com/deepviss/deepviss-server-example/blob/3ea50ce849c628c9cffa8657e4db9a663fd6b3d8/src/main/resources/deep-viss-openapi-v3.json",
					"serviceKind": "GITHUB"
				}
			]
		}
	},
	"Error": "parse: parse spec: parse components: schemas: \"ProcessingRequest\": parse schema: property \"image\": at 655:20: parse schema: unexpected schema type: \"Image\""
}