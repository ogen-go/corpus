{
	"File": {
		"__typename": "FileMatch",
		"repository": {
			"name": "github.com/Xilinx/inference-server"
		},
		"file": {
			"name": "rest_api.yaml",
			"size": 0,
			"path": "docs/rest_api.yaml",
			"byteSize": 15910,
			"content": "# Copyright 2022 Advanced Micro Devices, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nopenapi: 3.0.3\ninfo:\n  title: AMDInfer\n  description: |-\n    This is the AMD Inference Server's REST API based on the OpenAPI 3.0 specification.\n  license:\n    name: Apache 2.0\n    url: http://www.apache.org/licenses/LICENSE-2.0.html\n  version: '2.0'\nservers: []\ntags:\n  - name: health\n    description: Check the health of the inference server\n  - name: metadata\n    description: Metadata about the inference server\n  - name: models\n    description: Interact with models\npaths:\n  /v2/:\n    get:\n      summary: Server Metadata\n      tags: [\"metadata\"]\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/metadata_server_response'\n        '400':\n          description: Bad Request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/metadata_server_error_response'\n      operationId: get-v2\n      description: 'The server metadata endpoint provides information about the server. A server metadata request is made with an HTTP GET to a server metadata endpoint. In the corresponding response the HTTP body contains the [Server Metadata Response JSON Object](#server-metadata-response-json-object) or the [Server Metadata Response JSON Error Object](#server-metadata-response-json-error-object).'\n  /v2/hardware:\n    post:\n      summary: Has Hardware\n      tags: [\"metadata\"]\n      responses:\n        '200':\n          description: The server has the requested resource\n        '404':\n          description: The server does not have the requested resource or not enough of it\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/hardware'\n      operationId: post-hardware\n      description: 'The hardware endpoint provides information about what hardware exists and is accessible to the server. A request is made by making an HTTP POST to the hardware endpoint with the [Hardware Request JSON Object](#has-hardware-request-json-object)'\n  /v2/health/live:\n    get:\n      summary: Server Live\n      tags: [\"health\"]\n      responses:\n        '200':\n          description: OK\n      operationId: get-v2-health-live\n      description: The \"server live\" API indicates if the inference server is able to receive and respond to metadata and inference requests. The \"server live\" API can be used directly to implement the Kubernetes livenessProbe.\n  /v2/health/ready:\n    get:\n      summary: Server Ready\n      tags: [\"health\"]\n      responses:\n        '200':\n          description: OK\n      operationId: get-v2-health-ready\n      description: The \"server ready\" health API indicates if all the models are ready for inferencing. The \"server ready\" health API can be used directly to implement the Kubernetes readinessProbe.\n  /v2/models:\n    get:\n      summary: Models\n      tags: [\"metadata\", \"models\"]\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/model_list'\n      operationId: get-models\n      description: The \"models\" endpoint provides you a list of model endpoints that are currently available for metadata and inference\n  /v2/models/${MODEL_NAME}/ready:\n    parameters:\n      - schema:\n          type: string\n        name: MODEL_NAME\n        in: path\n        required: true\n    get:\n      summary: Model Ready\n      tags: [\"health\", \"models\"]\n      responses:\n        '200':\n          description: OK\n      operationId: get-v2-models-$-modelName-ready\n      description: The \"model ready\" health API indicates if a specific model is ready for inferencing. The model name and (optionally) version must be available in the URL. If a version is not provided the server may choose a version based on its own policies. Currently, version is not supported.\n  /v2/models/${MODEL_NAME}:\n    parameters:\n      - schema:\n          type: string\n        name: MODEL_NAME\n        in: path\n        required: true\n    get:\n      summary: Model Metadata\n      tags: [\"metadata\", \"models\"]\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/metadata_model_response'\n      operationId: get-v2-models-$-modelName\n      description: The per-model metadata endpoint provides information about a model. A model metadata request is made with an HTTP GET to a model metadata endpoint. In the corresponding response the HTTP body contains the [Model Metadata Response JSON Object](#model-metadata-response-json-object) or the [Model Metadata Response JSON Error Object](#model-metadata-response-json-error-object). The model name and (optionally) version must be available in the URL. If a version is not provided the server may choose a version based on its own policies or return an error. Version is currently not supported\n  /v2/models/${MODEL_NAME}/load:\n    parameters:\n      - schema:\n          type: string\n        name: MODEL_NAME\n        in: path\n        required: true\n    post:\n      tags: [\"models\"]\n      summary: Model Load\n      operationId: post-v2-models-$-MODEL_NAME-model-load\n      responses:\n        '200':\n          description: OK\n        '400':\n          description: Bad Request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/inference_error_response'\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/parameters'\n      description: Prior to inference, a model must be loaded to serve it. A model can be loaded with an HTTP POST request to the model load endpoint. The request consists of an optional set of parameters for the model. Model load expects that the model files are already available in the expected format in the \"model-repository\" directory for the running server.\n  /v2/models/${MODEL_NAME}/unload:\n    parameters:\n      - schema:\n          type: string\n        name: MODEL_NAME\n        in: path\n        required: true\n    post:\n      tags: [\"models\"]\n      summary: Model Unload\n      operationId: post-v2-models-$-MODEL_NAME-model-unload\n      responses:\n        '200':\n          description: OK\n        '400':\n          description: Bad Request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/inference_error_response'\n      description: A model can be unloaded with an HTTP POST request to the model unload endpoint. This is identical to 'worker unload'\n  /v2/workers/${WORKER_NAME}/load:\n    parameters:\n      - schema:\n          type: string\n        name: WORKER_NAME\n        in: path\n        required: true\n    post:\n      tags: [\"models\"]\n      summary: Worker Load\n      operationId: post-v2-models-$-WORKER_NAME-load\n      responses:\n        '200':\n          description: OK\n          content:\n            text/html:\n              example: '\u003chtml\u003eendpoint\u003c/html\u003e'\n        '400':\n          description: Bad Request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/inference_error_response'\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/parameters'\n      description: Prior to inference, a model must be loaded to serve it. A model can be loaded with an HTTP POST request to the worker load endpoint. The request consists of an optional set of parameters for the worker. Depending on the worker, some of these parameters may be required.\n  /v2/workers/${WORKER_NAME}/unload:\n    parameters:\n      - schema:\n          type: string\n        name: WORKER_NAME\n        in: path\n        required: true\n    post:\n      tags: [\"models\"]\n      summary: Worker Unload\n      operationId: post-v2-models-$-WORKER_NAME-worker-unload\n      responses:\n        '200':\n          description: OK\n        '400':\n          description: Bad Request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/inference_error_response'\n      description: A worker can be unloaded with an HTTP POST request to the worker unload endpoint. This is identical to 'model unload'\n  /v2/models/${MODEL_NAME}/infer:\n    parameters:\n      - schema:\n          type: string\n        name: MODEL_NAME\n        in: path\n        required: true\n    post:\n      tags: [\"models\"]\n      summary: Inference\n      operationId: post-v2-models-$-MODEL_NAME-infer\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/inference_response'\n        '400':\n          description: Bad Request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/inference_error_response'\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/inference_request'\n      description: 'An inference request is made with an HTTP POST to an inference endpoint. In the request the HTTP body contains the [Inference Request JSON Object](#inference-request-json-object). In the corresponding response the HTTP body contains the [Inference Response JSON Object](#inference-response-json-object) or [Inference Response JSON Error Object](#inference-response-json-error-object). See [Inference Request Examples](#inference-request-examples) for some example HTTP/REST requests and responses.'\n  /metrics:\n    get:\n      tags: [\"metadata\"]\n      summary: Metrics\n      operationId: get-v2-metrics\n      responses:\n        '200':\n          description: OK\n          content:\n            text/html:\n              example: '\u003chtml\u003emetrics...\u003c/html\u003e'\n      description: Get Prometheus-styled metrics from the server\ncomponents:\n  schemas:\n    metadata_server_response:\n      title: metadata_server_response\n      type: object\n      description: ''\n      x-examples: {}\n      properties:\n        name:\n          type: string\n        version:\n          type: string\n        extensions:\n          type: array\n          items:\n            type: string\n      required:\n        - name\n        - version\n        - extensions\n    metadata_server_error_response:\n      title: metadata_server_error_response\n      type: object\n      properties:\n        error:\n          type: string\n      required:\n        - error\n    metadata_model_response:\n      title: metadata_model_response\n      type: object\n      properties:\n        name:\n          type: string\n        versions:\n          type: array\n          items:\n            type: string\n        platform:\n          type: string\n        inputs:\n          type: array\n          items:\n            $ref: '#/components/schemas/metadata_tensor'\n        outputs:\n          type: array\n          items:\n            $ref: '#/components/schemas/metadata_tensor'\n      required:\n        - name\n        - platform\n    metadata_tensor:\n      title: metadata_tensor\n      type: object\n      properties:\n        name:\n          type: string\n        datatype:\n          type: string\n        shape:\n          type: array\n          items:\n            type: integer\n      required:\n        - name\n        - datatype\n        - shape\n    metadata_model_error_response:\n      title: metadata_model_error_response\n      type: object\n      properties:\n        error:\n          type: string\n      required:\n        - error\n    inference_request:\n      title: inference_request\n      type: object\n      x-examples:\n        Example 1:\n          id: '42'\n          inputs:\n            - name: input0\n              shape:\n                - 2\n                - 2\n              datatype: UINT32\n              data:\n                - 1\n                - 2\n                - 3\n                - 4\n            - name: input1\n              shape:\n                - 3\n              datatype: BOOL\n              data:\n                - true\n          outputs:\n            - name: output0\n        Example 2:\n          id: '42'\n          outputs:\n            - name: output0\n              shape:\n                - 3\n                - 2\n              datatype: FP32\n              data:\n                - 1\n                - 1.1\n                - 2\n                - 2.1\n                - 3\n                - 3.1\n      properties:\n        id:\n          type: string\n        parameters:\n          $ref: '#/components/schemas/parameters'\n        inputs:\n          type: array\n          items:\n            $ref: '#/components/schemas/request_input'\n        outputs:\n          type: array\n          items:\n            $ref: '#/components/schemas/request_output'\n      required:\n        - inputs\n    parameters:\n      title: parameters\n      x-examples: {}\n      type: object\n      items:\n        anyOf:\n          - type: number\n          - type: string\n          - type: boolean\n    request_input:\n      title: request_input\n      type: object\n      properties:\n        name:\n          type: string\n        shape:\n          type: array\n          items:\n            type: integer\n        datatype:\n          type: string\n        parameters:\n          $ref: '#/components/schemas/parameters'\n        data:\n          $ref: '#/components/schemas/tensor_data'\n      required:\n        - name\n        - shape\n        - datatype\n        - data\n    tensor_data:\n      title: tensor_data\n      type: array\n      items:\n        anyOf:\n          - $ref: '#/components/schemas/tensor_data'\n          - type: number\n          - type: string\n          - type: boolean\n    request_output:\n      title: request_output\n      type: object\n      properties:\n        name:\n          type: string\n        parameters:\n          $ref: '#/components/schemas/parameters'\n      required:\n        - name\n    response_output:\n      title: response_output\n      type: object\n      properties:\n        name:\n          type: string\n        shape:\n          type: array\n          items:\n            type: integer\n        datatype:\n          type: string\n        parameters:\n          $ref: '#/components/schemas/parameters'\n        data:\n          $ref: '#/components/schemas/tensor_data'\n      required:\n        - name\n        - shape\n        - datatype\n        - data\n    inference_response:\n      title: inference_response\n      type: object\n      properties:\n        model_name:\n          type: string\n        model_version:\n          type: string\n        id:\n          type: string\n        parameters:\n          $ref: '#/components/schemas/parameters'\n        outputs:\n          type: array\n          items:\n            $ref: '#/components/schemas/response_output'\n      required:\n        - model_name\n        - outputs\n    inference_error_response:\n      title: inference_error_response\n      type: object\n      properties:\n        error:\n          type: string\n    hardware:\n      title: hardware\n      type: object\n      properties:\n        name:\n          type: string\n        num:\n          type: integer\n      required:\n        - name\n        - num\n    model_list:\n      title: model_list\n      type: array\n      items:\n        type: string\n",
			"canonicalURL": "/github.com/Xilinx/inference-server@28b3c8fae35b019d581dfda5f562fabe53eda28c/-/blob/docs/rest_api.yaml",
			"externalURLs": [
				{
					"url": "https://github.com/Xilinx/inference-server/blob/28b3c8fae35b019d581dfda5f562fabe53eda28c/docs/rest_api.yaml",
					"serviceKind": "GITHUB"
				}
			]
		}
	},
	"Error": "parse: parse spec: parse components: schemas: \"request_input\": resolve \"#/components/schemas/request_input\": property \"parameters\": $ref: resolve \"#/components/schemas/parameters\": at docs/rest_api.yaml:407:9: items: object cannot contain 'items' field"
}